\documentclass[12pt,english,a4paper,titlepage,cleardoublepage=empty,dottedtoc]{report}



% % % additions by tompollard START -->

% Overwrite \begin{figure}[htbp] with \begin{figure}[H]
\usepackage{float}
\let\origfigure=\figure
\let\endorigfigure=\endfigure
\renewenvironment{figure}[1][]{%
\origfigure[b]
}{%
\endorigfigure
}

% TP: hack to truncate list of figures/tables.
\usepackage{truncate}
\usepackage{caption}
\usepackage{tocloft}
% TP: end hack

% % % <-- END additions by tompollard



\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Master Thesis: Open Specification of a user-controlled Web Service for Personal Data},
            pdfauthor={G. Jahn},
            pdfkeywords={personal data store; personal data as a service; web service architecture; open specification; masters thesis},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi
% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother



% Table of contents formatting
\renewcommand{\contentsname}{Table of Contents}
\setcounter{tocdepth}{3}

% Headers and page numbering
\usepackage{fancyhdr}
\pagestyle{plain}

% Table package
\usepackage{ctable}% http://ctan.org/pkg/ctable

% Deal with 'LaTeX Error: Too many unprocessed floats.'
\usepackage{morefloats}
% or use \extrafloats{100}
% add some \clearpage

% % Chapter header
% \usepackage{titlesec, blindtext, color}
% \definecolor{gray75}{gray}{0.75}
% \newcommand{\hsp}{\hspace{20pt}}
% \titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}

% % Fonts and typesetting
% \setmainfont[Scale=1.1]{Helvetica}
% \setsansfont[Scale=1.1]{Verdana}

% FONTS
\usepackage{xunicode}
\usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text} % converts LaTeX specials (``quotes'' --- dashes etc.) to unicode
% \setromanfont[Scale=1.01,Ligatures={Common},Numbers={OldStyle}]{Palatino}
% \setromanfont[Scale=1.01,Ligatures={Common},Numbers={OldStyle}]{Adobe Caslon Pro}
%Following line controls size of code chunks
% \setmonofont[Scale=0.9]{Monaco}
%Following line controls size of figure legends
% \setsansfont[Scale=1.2]{Optima Regular}

%Attempt to set math size
%First size must match the text size in the document or command will not work
%\DeclareMathSizes{display size}{text size}{script size}{scriptscript size}.
\DeclareMathSizes{12}{13}{7}{7}

% ---- CUSTOM AMPERSAND
% \newcommand{\amper}{{\fontspec[Scale=.95]{Adobe Caslon Pro}\selectfont\itshape\&}}

% HEADINGS
\usepackage{sectsty}
\usepackage[normalem]{ulem}
\sectionfont{\rmfamily\mdseries\Large}
\subsectionfont{\rmfamily\mdseries\scshape\large}
\subsubsectionfont{\rmfamily\bfseries\upshape\large}
% \sectionfont{\rmfamily\mdseries\Large}
% \subsectionfont{\rmfamily\mdseries\scshape\normalsize}
% \subsubsectionfont{\rmfamily\bfseries\upshape\normalsize}

% Set figure legends and captions to be smaller sized sans serif font
\usepackage[font={footnotesize,sf}]{caption}

\usepackage{siunitx}

% Adjust spacing between lines to 1.5
\usepackage{setspace}
% \onehalfspacing
\doublespacing
\raggedbottom

% Set margins
\usepackage[top=1.5in,bottom=1.5in,left=1.5in,right=1.4in]{geometry}
% \setlength\parindent{0.4in} % indent at start of paragraphs (set to 0.3?)
\setlength{\parskip}{9pt}

% Add space between pararaphs
% http://texblog.org/2012/11/07/correctly-typesetting-paragraphs-in-latex/
% \usepackage{parskip}
% \setlength{\parskip}{\baselineskip}

% Set colour of links to black so that they don't show up when printed
\usepackage{hyperref}
\hypersetup{colorlinks=false, linkcolor=black}

% Tables
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{array}
\newcolumntype{x}[1]{%
>{\centering\arraybackslash}m{#1}}%

% Allow for long captions and float captions on opposite page of figures
% \usepackage[rightFloats, CaptionBefore]{fltpage}

% Don't let floats cross subsections
% \usepackage[section,subsection]{extraplaceins}


% Chapter styling
% src: https://github.com/chiakaivalya/thesis-markdown-pandoc/blob/master/preamble.tex#L26
\usepackage[grey]{quotchap}
\makeatletter
\renewcommand*{\chapnumfont}{%
  \usefont{T1}{\@defaultcnfont}{b}{n}\fontsize{80}{100}\selectfont% Default: 100/130
  \color{chaptergrey}%
}
\makeatother

\title{Master Thesis: Open Specification of a user-controlled Web Service for
Personal Data}
\author{G. Jahn}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
Data is the currency of tomorrow. Organizations, whether in the private
or public sector, are gathering enormous amounts of personal (big) data.
This data is harvested and incorporated by these third parties, but were
created by individuals and should, therefore, belong to them. People are
depending on their data. Their identity as well as their personality are
defined by their personal data. Meanwhile data silo operators are
hammering onto these haystacks eagerly trying to find any correlations
worth interpreting, thereby almost inevitably discriminating against the
rightful owners. To reduce the possibility of discrimination only bare
minimum of data required should be handed over to a third party. Thus
the individual has to be in charge of the whole process. A personal data
service will empower its user to regain full control over her data and
facilitates detailed information on every data flow. To be able to trust
such a tool, the user should be able to look inside. Therefore a
personal data service has to be open source and developed transparently,
which would then also encourage self-hosting.
\end{abstract}

{
\setcounter{tocdepth}{1}
\tableofcontents
}
This page intentionally left blank

\chapter{Introduction}\label{introduction}

\section{Motivation}\label{motivation}

Nowadays it is rare to find someone that does not collect data about
some kind of thing; particularly humans are the targets of choice for
the \emph{Big Data Movement}
{[}\protect\hyperlink{ref-web_2016_privacy-international-about-big-data}{1}{]}.
Since humans are all individuals, they are - more or less - distinct
from each other. However, subsets of individuals might share a minor set
of attributes, but the bulk is still very unique to an individual, given
that the overall variety of attributes is fairly complex. That small
amount of shared attributes might seem to be less important, due to the
nature of inflationary occurrence, but the opposite turns out to be
true. These similarities allow to determine the individuals who are part
of a subset and the ones who arn't. Stereotypical patterns are applied
to these subsets and thus to all relating individuals. Thus enriched
information are then used to help predicting outcomes of problems or
questions regarding these individuals. In other words, searching for
causation where in best the case one might find correlations - or so
called \emph{discrimination}, which

\begin{quote}
{[}\ldots{}{]} refers to unfair or unequal treatment of people based on
membership to a category or a minority, without regard to individual
merit.
\emph{{[}\protect\hyperlink{ref-paper_2008_discrimination-aware-data-mining}{2}{]}}
\end{quote}

When interacting directly with each other, discrimination of human
beings is still a serious issue in our society, but also when humans
leverage computers and algorithms to uncover formerly unnoticed
information in order to include them in their decision making. For
example when qualifying for a loan, hiring employees, investigating
crimes or renting flats. Approval or denial, the decision is based on
computed data about the individuals in question
{[}\protect\hyperlink{ref-book_2015_ethical-it-innovation_ethical-uses-of-information-and-knowledge}{3}{]},
which is simply discrimination on a much larger scale and with less
effort - almost parenthetically. The described phenomenon is originally
referred to as \emph{Bias in computer systems}
{[}\protect\hyperlink{ref-paper_1996_bias-in-computer-systems}{4}{]}.
What at first seems like machines going rouge on humans, is, in fact,
the \emph{cognitive bias}
{[}\protect\hyperlink{ref-wikipedia_2016_cognitive-bias}{5}{]} of human
nature, modeled in machine executable language and made to reveal the
patterns their creators were looking for - the \emph{``Inheritance of
humanness''}
{[}\protect\hyperlink{ref-web_2016_big-data-is-people}{6}{]} so to say.

In addition to the identity-defining data mentioned above, humans have
the habit to create more and more data on a daily basis - pro-actively
(e.g by writing a tweet) and passively (e.g by allowing the twitter app
accessing their current location while submitting the tweet). As a
result, already tremendous amounts of data keep growing bigger and
bigger, waiting to be harvested, collected, aggregated, analyzed and
finally interpreted. The crux here is, the more data being made
available
{[}\protect\hyperlink{ref-video_2015_big-data-and-deep-learning_discrimination}{7}{]}
to \emph{mine}, the higher the chances to isolate data sets, that differ
from each other but are coherent in themselves. Then it is just a matter
of how to distinguish the data set and thereby the related individuals
from each other.

In order to lower potential discrimination we either need to erase
responsible parts from the machines, thereby it's crucial raising
awareness and teaching people about the issue of discrimination, or we
try to prevent our data from falling into these data silos. The latter
will be addressed in this work.

\section{Purpose \& Outcome}\label{purpose-outcome}

From an individual's perspective providing data to third parties might
not seem harmful at all. Instead eventually one get improved services in
return, e.g.~more adequate recommendations and fitting advertisement, or
more helpful therapies and more secure environments. That said, though
it is a matter of perception what's good and bad, what's harmful and
what's an advantage. Computing data to leverage decision making is
essentially just science and technology and it's up to the humans how
such tools are getting utilized and what purposes they are serving.
Hence it should be decided by the data creators, how their data get
processed and what parts of them are used.

To tackle the described issue the initial idea here is (1) to equip
individuals with the ability to control and maintain their entire data
distribution and (2) thus reducing the amount of \emph{potentially
discriminatory}
{[}\protect\hyperlink{ref-paper_2008_discrimination-aware-data-mining}{2}{]}
attributes leaking into arbitrary calculations. To do so people need a
reliable and trustworthy tool, which assists them in managing all their
\emph{personal data} and making them accessible for 3rd parties but
under their own conditions. After getting permission granted these data
consumers might have the most accurate and reliable one-stop resource to
an individuals's data at hand, while urged to respect their privacy at
the same time. However this also comes with downsides in terms of
security and potential data loss. Elaborating on that and discussing
different solutions will be part of the {[}design
process{]}{[}Design{]}.

The way how to solve the described dilemma is not new. Early days of
work done in this field can be dated back to the Mid-2000s where studies
were made e.g.~about recent developments in the industry or user's
concerning about privacy, and the term \emph{Vendor Relationship
Management (VRM)} were used initially within the context of user-centric
personal data management, which also led into the \emph{ProjectVRM}
{[}\protect\hyperlink{ref-web_2010_projectvrm_about}{8}{]} started by
the \emph{Berkman Klein Center for Internet \& Society at Hardvard
University}. Since then a great amount of effort went into this research
area until today, while also commercial products and business models
trying to solve certain problems. For instance concepts such as the
\emph{Personal Data Store (PDS)}
{[}\protect\hyperlink{ref-paper_2013_the-personal-data-store-approach-to-personal-data-security_2013}{9}{]}
or a \emph{MyData}
{[}\protect\hyperlink{ref-whitepaper_2014_mydata-a-nordic-model-for-human-centered-personal-data-management-and-processing}{10}{]}
implementation called \emph{Meeco}
{[}\protect\hyperlink{ref-web_2016_meeco-how-it-works}{11}{]}, which
will all be covered in a more detailed way within the following chapter.

The work and research done for this thesis will be the foundation for an
\emph{Open Specification}, which by itself is a manual to implement a
concept called \emph{Personal Data as a Service}. Important topics like
how the architecture will look like, where the actual data can be
stored, how to obtain data from the external API or what requirements a
user interface for data management need to satisfy, will be examined.
After the thesis will be finished, the majority of core issues should
already be addressed and can then get outlined in the specification
document. Only then the task to actual implement certain components can
begin. The reason for that is, when sensitive subjects especially like
people's privacy is at risk, all aspects in question deserve a careful
considerations and then get addressed properly. Thus it is indispensable
to put adequate effort primarily into the theoretical work. To be clear
though, that doesn't mean writing code to test out theories and ideas
can't be done during research and specification development. It might
even help to spot some flaws and eventually trigger evolvement.

To ensure a great level of trust to this project and the resulting
software, it is vital to make the development process fully transparent
and encourage people to get involved. Therefore it is required to open
source all related software and documents
{[}\protect\hyperlink{ref-repo_2016_pdaas-spec}{12}{]} from day one on.

In summary, this document is meant to be the initial step in a
development process fabricating a tool to manage all data defining a
data subject's identity, that is controlled and administrated by that
individual, so that maybe she is giving a more precise understanding
about where her personal information flows and how this might effect her
privacy.

\section{Scenarios}\label{scenarios}

The following use cases shall depict different situations and possible
ways such emerging software might be applicable or useful, while
providing it's user with more control over her personal data. Some of
them are more practical and realistic, like ordering and purchasing
online a product, others might have no current usage, but showing a
certain potential to become more relevant when new technologies and
business models emerge, followed by new demands of data.

\subsubsection{Ordering a product
online}\label{ordering-a-product-online}

The data subject searches through the web to find a new toaster, since
her old one recently broke. After some clicks and reviews, she found her
soon-to-become latest member of the household's kitchenware. After
putting the model name in a price search engine, hoping to save some
money, the first entry, offering a 23\% discount, caught her attention.
She decides to have a deeper look into the toasters and thus has heading
towards the original web shop entry. Finally she came around and put the
item onto her card, despite tha fact, that she has never bought
something from that online shop before. Then she proceeded to checkout
to place her order. The shop-interface is asking her to either insert
her credentials, proceed without registration or sign-in, or insert s
URI to an endpoint of her \emph{Personal Data as a Service}. TODO: the
following description might need some adjustments according data flow /
process description She opens up the management panel of her
\emph{PDaaS} and creates a new entry in a list of data consumers, that
already have access to characteristics of her personal data. As a
result, she receives a URI, which she inserts according, as mentioned
before; after she assures herself that the data exchange with the shop
through the browser is based on a secure connection (HTTPS). Under this
URI, the shop-system can then request data, that is required for a
successful transaction. Moving on to the next step after submitting the
URI, the data subject is ask to decide how she would like to pay. The
choices are: credit card, invoice, online payment provider of choice or
bank transfer. She chooses the last one, submits her selection and
thereby completes her order. After a moment, a push notification pops up
on her mobile device, which is a permission request from her
\emph{PDaaS}, asking for granting the shop-system, she just places the
order, access to her full name, address and email. Additionally she can
decide between three states of how long the permission wil be valid:
\emph{only one time}, \emph{expires on date} and \emph{granted, until
further notice}. Since she never ordered at this shop before and might
never again, she decided to grant access only for this specific
occasion. After the shop-system receives the data, it sends an email to
the data subject, containing some information about her order, including
the shop's bank details. which then enables her to actually pay the
amount due. After the system recognizes the payment has coming in, it
triggers the shipment of the toaster. In order to get a full impression
of how the whole process might have look like when the data subject had
chosen one of the other payment methods, the differences will be
describes in the following. If the data subject would have wanted to pay
with her credit card, the only difference would have been, that the
shop-system had requested also to access the credit card number and it's
belonging secret, and when sending the email the system would have
omitted the information about the shop's bank details. Being able to
choose paying with invoice where possible only because the \emph{PDaaS}
response has indicated, that it's containing \emph{profile data} is
certified and therefore trustworthy. Which reduces the shop owner's risk
and would have enabled him in case of fraud or misuse to take action.
Choosing to involve paypal as a \emph{middleman} to process the payment,
requires the data subject to had already granted paypal certain access
to her \emph{PDaaS}. If that's the case, then the shop-system would have
ask also for her paypal-ID, which then the system will use to request
the payment directly from paypal. This on the other side will cause
paypal to consult the \emph{PDaaS}, which results in a second
notification, asking the data subject for permission to proceed. After
the payment transfer was successful, the shipment will gets initiated.
And with the package arriving at the data subject's doorstep the whole
transaction has finished.

\subsubsection{Interacting with a social
network}\label{interacting-with-a-social-network}

Entering a social network for the first time, only take the URI to the
data subject's \emph{PDaaS} and a password. The data subject receives a
notification on her mobile device asking for permission to access
certain data about her. If her mobile device is currently not at hand,
she can also use the administration panel provided by her \emph{PDaaS}
and reachable with a web browser on every internet-enabled device.
Within that panel pending permission reviews will be indicated. Whether
the data subject has already reviewed the request or not, she should be
able to login to the social network. After doing so, she should not be
able to see any of her information. After granting permissions to the
social network to accessing certain data \emph{until-further-notice} and
reloading the session, she then should see all her So every time,
someone on that network tries to access her information, whom she has
allowed to see that information (which is managed by the user only from
within the network), the network pulls the data from the data subjects's
\emph{PDaaS}, if it's still permitted to do so. It's also imaginable,
that the social network and a \emph{PDaaS} are establishing a backward
channel. This channel could be used to send all the content she would
create over time while interacting with the social network and it's
participants back to her \emph{PDaaS}. The network itself only stores a
reference to all content object, whether it's for example an image, a
post or comment on somebody else's post and if it's needed the actual
content will be fetched from the data subject's \emph{PDaaS}.

\subsubsection{Applying for a loan and checking
creditworthiness}\label{applying-for-a-loan-and-checking-creditworthiness}

The data subject would like to buy an apartment. In order to finance
such a acquisition, she needs a funding, which in her case, will be
based on a loan. During a conversation in a credit institute of her
choice, an account consultant describes to her what data will be
required in order to decide about her creditworthiness. While giving a
consensual nod, she takes out her smartphone and brings up the
management panel of her \emph{PDaaS}. With a few taps she has just
created a new \emph{data consumer}. The panel then shows a QR-Code, that
holds a URI to a dedicated endpoint of the data subject's \emph{PDaaS}.
She shows that code to her consultant, who then scans it. While handling
some more formalities and talking about several issues and possible
products she might be interested it, she gets a notification on her
phone, informing her about a permission request the institute just made.
It lists all the different data points the institute would like to
access in order to calculate her scoring, such as address, monthly
income, relationship status and family, history of banking or other
current loans. After some back and forth and solving some
misunderstandings with the help of her consultant, she decided to just
partially allow access to the requested data and just for this time and
purpose. The consultant kindly pointed out, that these decisions might
have an impact on the scoring and thereby on the lending and it's terms.
After the consultant got a signal from the computer system, the two then
finishing up their meeting and the consultants informed the data data
subject about the next steps, which includes a note, that the institute
will contact her within the next few days, when they have come to a
conclusion. In case of a positive outcome a new appointment need to be
made, for doing all the paperwork and signing the contract. From a
technical point of view, two different ways of computing the score are
imaginable. The first one would be, transferring only the plain data -
request, containing the query and response containing the data -
including the expire date and information regarding the signature state.
But the actual computations and analytics to obtain the score, will
happen within the infrastructure of the credit institute. When this
process is over, all transferred personal data has to be deleted. An
alternative could prevent the data from leaving the \emph{PDaaS}, in
which the institute's request won't consists of a data query. Instead it
would came along with an chunk of software and some information on how
to run it. The \emph{PDaaS} server will provide an isolated runtime in
which the software then gets executed. After the process has finished,
the result will be send back to the credit institute's infrastructure.

\subsubsection{Maintain and provide it's own health/patient
record}\label{maintain-and-provide-its-own-healthpatient-record}

Some time ago on a hiking trip in a moment of carelessness the data
subject has accidently broke her leg. She came into a hospital and went
straight into surgery, where the physicians could fix the injury. Time
went by and the leg has healed completely. After she woke up today she
felt some pain coming from that area where her leg was broken. She
decided to call in sick and went straight to a doctor nearby. During her
recovery she visited that doctor regularly. At the reception desk, she
opens up the \emph{PDaaS}'s management panel on her smartphone and
searched through the list of data consumers. After she found the entry
for this clinic, she flipped her phone to show the receptionist the
corresponding QR-Code, which she started to scans immediately. However
the receptionist couldn'd see any data on the screen, because the access
has already expired. The data subject only had permitted access for the
estimated time of recovery, which was over some time ago. That's why she
got a notification, to re-grant some access. Going through the data
points the clinic-system has requested, she noticed that her address is
incorrect. Last month she moved out and into a bigger apartment just
down the street. She must have forgot to change that data, which she
corrects immediately right before submitting the access configurations
for the clinic-system. She also included the access to all the data
originated from that time after her accident. A moment later the
receptionist confirms to now being able to see all necessary data. The
data subject takes a seat in the waiting room. While passing some time,
she had a deeper look into her list of data consumers; some of them she
couldn't even remember and for others she was surprised to what data she
has granted access to and started to reduce certain permissions, if it
was appropriate in her eyes. She even removed some of the entries. The
appointment with her doctor went great. He even had to review the x-ray
images in order to make a adequate differential diagnosis. After the
visit, she had to make a quick stop at a pharmacy along the way to
pickup the drugs her doctor had prescribed for her to reduce the pain.
She had to wait in the queue with two other customers being in front of
her. She realized, that it's the first time she has been here. So she
prepared a new entry in her data consumer list, including all
information about her prescriptions. So by the time she get served, she
just let the person behind the register scan her code. In the next
seconds the data subject gets a quick confirmation notification about
the request that just happened. A moment later the pharmacist come back
with her drugs, which she then pays in cash and the transaction is done.

\subsubsection{Vehicle data and
mobility}\label{vehicle-data-and-mobility}

Assuming a car itself has no hardware on board in order to establish a
wireless wide area connection to an outside access node. Only from the
inside one can connect to the car (wired or wireless). After entering a
car, on the data subject's mobile device pops up a notification asking
for permission to connect to that device. In addition to the expiration
date, the data subject can choose to en- or disable two more options.
First, a wifi network with an uplink to the internet can be provided to
everyone inside the car. Secondly, connections, the car might want to
establish, in order to emit data via internet - which, regardless, have
to go through the currently linked mobile device. Thus the device owner
gains full control over any external data transfer that might happen.
This again would allow two things: (A) permission management for all
outgoing data and (B) funnel all data generated and provided by the car
into the \emph{PDaaS} associated with that linked device. It might also
be feasible to deny any connection the car is trying to make. Thus the
data will only be stored in the \emph{PDaaS}. If somebody is interested
in such then have to ask for access permission. That same concept about
movement tracking and vehicle data could also be applied to driving
(motor) bicycle.

\hypertarget{terminologies}{\section{Terminologies}\label{terminologies}}

\begin{description}
\tightlist
\item[Web Service]
TODO
\item[Open Specification]
TODO
\item[Big Data]
deep learning, neural networks
\item[Profile Data]
individual's inherent data; TODO
\item[Personal Data (TODO)]
Personal Information predominantly static data points related to an
individual
\item[Personal Data as a Service (PDaaS)]
a web service controlled, owned and maybe even hosted by an individual,
that provides access to the data subject's personal data and offers
maintainability as well as permission management. It can be seen as her
personal agent; sometimes also referred to as \emph{the system}
\item[Personal Data Store]
TODO
\item[Vendor Relationship Manager]
The \emph{ProjectVRM} defines the a VRM as follows: ``TODO''
{[}\protect\hyperlink{ref-web_2010_projectvrm-wiki_about-vrm}{13}{]}
\item[Personal Information Management Systems (PIMS)]
TODO
{[}\protect\hyperlink{ref-web_2010_projectvrm-wiki_pims-example-list}{14}{]}
\item[serverless]
TODO https://auth0.com/blog/2016/06/09/what-is-serverless/
\item[Digital Footprints]
TODO
\item[Data Subject]
an individual who first and foremost is the owner of all of her personal
data; sometimes referred to as \emph{owner}
\item[\protect\hypertarget{terminologies--operator}{}{Operator}]
a \emph{data subject} using a PDaaS to control (and probably host) her
personal data; sometimes referred to as \emph{data controller}
\item[\protect\hypertarget{terminologies--consumer}{}{(Data) Consumer}]
Third party, external entity requesting data, authorized by the data
subject to do so; sometimes referred to as \emph{(data) collector}
\item[Data Broker(s)]
entities with commercial interests, that collect, aggregate and analyze
information/data of any kind - in this case about human beings - from
different sources in order to enrich the data sets, to finally license
the resulting corpora to other organisations.
{[}\protect\hyperlink{ref-report_2014_data-brokers}{15}{]}
\item[Permission Request]
initial attempt to request permissions for accessing certain data from
the \emph{PDaaS}; third party registers as \emph{data consumer}
\item[Access Request]
obtain/request actual data from the system (requires a third party to be
registered as a \emph{data consumer})
\item[Permission Profile]
a data set about a third party that already made a permission request.
The set contains additional information and access rules
\item[Data Access]
after a third party's \emph{permission request} got reviewed and saved,
that entity is then able to make an attempt to access data.
\item[Endpoint]
an endpoint is defined as the part of the URI that is unique to every
\emph{data consumer}, or to be more precise, unique to every
\emph{permission profile}. Usually it is the first part of a URI,
whereas following parts stand for different resources that might be
available within that endpoint It can also be seen as group of resources
that all can be accessed by under specific circumstances
\end{description}

\chapter{Fundamentals}\label{fundamentals}

The following chapter shall provide the foundational knowledge about
concepts like \emph{Personal Identity} or \emph{Big Data} and therefore
ensures a common understanding on their relation to the problem this
work tries to solve. Additionally it is given a brief overview on what
existing standards and technologies might be used, and summarizes the
research already been made as well as it's current state.

\hypertarget{digital-identity-personal-data-and-ownership}{\section{Digital
Identity, Personal Data and
Ownership}\label{digital-identity-personal-data-and-ownership}}

\begin{itemize}
\tightlist
\item
  \emph{Digital Identity}

  \begin{itemize}
  \tightlist
  \item
    what is a \emph{DI}? and in comparison to \emph{Personal Data}?
  \item
    what is required to make the PDaaS used or seen as a \emph{DI}?
  \end{itemize}
\item
  \emph{Personal Data} definition

  \begin{itemize}
  \tightlist
  \item
    general - freely spoken
  \item
    as of EU law (incl citation)
  \item
    as of US law (incl citation)
  \item
    is it just policy/guideline or enforceable too (law/rule)? what
    relevance/impact have companies \emph{terms and conditions}?
  \item
    EU and USA (since server might be located outside the state or
    effective range)
  \end{itemize}
\item
  \emph{Ownership} of personal data

  \begin{itemize}
  \tightlist
  \item
    who is the owner in what situation or under what circumstances?
  \item
    am I the owner when I was the one who was collecting them? Does it
    depend on whether the resource was public or somewhat private?
  \item
    what will happen with her data service after a person died?
  \end{itemize}
\end{itemize}

\begin{itemize}
\item
  A \textbf{Digital Identity} is a non-physical abstraction of an
  entity, such as an organisation, an individual, a device or even
  software, which allows bidirectional association. In the context of
  this document, it only refers to human beings. Therefore a
  \emph{digital identity} is the individual's representation in digital
  systems, consisting of identity-defining data, such as \emph{personal
  information} and it's history and preferences
  {[}\protect\hyperlink{ref-whitepaper_2012_the-value-of-our-digital-identity_definition}{16}{]}.
  \emph{Personal information}, in this case, refers to inherent (date of
  birth) and imposed (credit card number) characteristics.
\item
  From a technical perspective a DI is essentially a collection of
  characteristics, attributes and time series data (e.g.~interaction
  logs or bank account history). A subset of those attributes combined
  can form unique fingerprint, like certain single data points
  (e.g.~social security number) in their own context might be, too. Thus
  it might not be necessary to know the values of all attributes in
  order to identify a person as the rightful owner and physical
  counterpart. It can also be seen as an avatar in the digital world or
  as the digital part of a human's identity. Therefore its important to
  not view the \emph{DI} as a reduction of a living individual to some
  bits and bytes, but rather as a appropriate representation for certain
  purposes and contexts.
\item
  It is also possible to provide an additional level of authenticity
  insurance for data related to an entity. Therefor an unrelated third
  party, which needs to be approved not only by the related individual,
  but also by all entities participating in a context, which might be
  relevant e.g.~for some administration purposes.
\item
  But the concept would also impose a new level of attacking vectors to
  the identity owner, such as identity theft. The attacker is no longer
  required to be physically present to be able to steal certain unique
  identifiers from a person. It is sufficient to gain access to area
  where the sensitive data is stored.
\item
  In the context of this document and all related work, \textbf{Personal
  Data} is specified as a combination of an individual's \emph{Digital
  Identity} and all of it's ever created intellectual property
  {[}\protect\hyperlink{ref-web_2016_wikipedia_intellectual-property}{17}{]}
  (e.g.~posts, images, tweets or comments). This includes all sorts of
  tracking data and interaction monitoring, as well as metadata manually
  or automated enriching content (e.g.geo-location attached to a tweet
  as meta information). Data, captured by someone ore something on or
  about the individual's private living space and property. Simply every
  data point reflecting the individual's personality - partly or as a
  whole - is seen as \emph{personal data}.
\item
  The european \emph{Data Protection Regulations} defining
  \emph{Personal Data} as follows: \textgreater{} `personal data' means
  any information relating to an identified or identifiable natural
  person \textgreater{} (`data subject'); an identifiable natural person
  is one who can be identified, directly or \textgreater{} indirectly,
  in particular by reference to an identifier such as a name, an
  identification \textgreater{} number, location data, an online
  identifier or to one or more factors specific to the physical,
  \textgreater{} physiological, genetic, mental, economic, cultural or
  social identity of that natural person; \textgreater{}
  \emph{{[}\protect\hyperlink{ref-regulation_2016_eu_general-data-protection-regulation_definition}{18}{]}}
\item
  The U.S.A. has little legislation on defining and protecting
  consumer's privacy. At least they have no explicit bills addressing
  such area
  {[}\protect\hyperlink{ref-web_2016_wikipedia_information-privacy-law_us}{19}{]}.
  Though some of the existing sectoral laws consist of partially
  applicable policies and guidelines
  {[}\protect\hyperlink{ref-web_2016_data-protection-laws-in-the-us}{20}{]};
  most of them addressing specific types of data. In 2015 the White
  House made an attempt to fill the gap with the \emph{Consumer Privacy
  Bill of Rights Act}, but to this date it didn't passes the draft
  state. According to the critics, it lags of concrete enforceable rules
  consumers can rely on
  {[}\protect\hyperlink{ref-web_2015_white-house-releases-consumer-privacy-bill-draft}{21}{]}.
  The draft contains a general definition of \emph{Personal Data}:
  \textgreater{} ``Personal data'' means any data that are under the
  control of a covered entity, not otherwise \textgreater{} generally
  available to the public through lawful means, and are linked, or as a
  practical matter \textgreater{} linkable by the covered entity, to a
  specific individual, or linked to a device that is \textgreater{}
  associated with or routinely used by an individual, including but not
  limited to {[}\ldots{}{]} \textgreater{}
  \emph{{[}\protect\hyperlink{ref-bill-draft_2015_us_consumer-privacy-bill-of-rights-act_definition}{22}{]}}
\item
  followed by a list of concrete data points, e.g.~email or postal
  address, name, social security number and alike. Aside from the
  legislation with bills, a few third-party organisation can also
  participate by and add new or overwriting existing rules and policies.
  Namely for example the \emph{Federal Communications Commission} (FCC),
  recently releasing \emph{Rules to Protect Broadband Consumer Privacy}
  including a list of categories of sensitive information
  {[}\protect\hyperlink{ref-rules_2016_fcc_to-protect-broadband-consumer-privacy_sensitive-types-of-data}{23}{]},
  which wants \emph{Personally Identifiable Information} (alias Personal
  Data) to be understood as: \textgreater{} {[}\ldots{}{]} any
  information that is linked or linkable to an individual.
  {[}\ldots{}{]} information is \textgreater{} ``linked'' or
  ``linkable'' to an individual if it can be used on its own, in
  context, or in \textgreater{} combination to identify an individual or
  to logically associate with other information about a \textgreater{}
  specific individual. \textgreater{}
  \emph{{[}\protect\hyperlink{ref-rules_2016_fcc_to-protect-broadband-consumer-privacy_personally-identifiable-information}{24}{]}}
\item
  Despite minor difference in detail, they all have similar ideas of
  personal data and their belonging. Even though, the version proposed
  by EU is almost identical with the definition introduced for the
  context of this work. Although the FCC's statutory authorities might
  be somewhat debatable regarding certain topics, the
  \emph{Communications Act} as a U.S. federal law equips the FCC with
  power to regulate and legislate.
\item
  Having a common opinion on what data points are belonging to person is
  the foundation to define a set of rules on how deal with
  \emph{Personal Data} accordingly. Every business, operating within the
  EU, is required\footnote{according to article 12-14 of the \emph{EU
    General Data Protection Regulation 2016/679}} to provide it's users
  with a \emph{Privacy Policy}, while e.g.~in the U.S. - as mentioned
  above - only partially and depending on context and data type users
  must be informed about which and how their data get processed
  {[}\protect\hyperlink{ref-web_2016_privacy-policies-are-mandatory-by-law}{25}{]}.
\item
  A user commonly agrees on the privacy policy, by starting to interact
  with the author's business, thus every \emph{Privacy Policy} is
  required to be publicly accessible; e.g. before creating an account.
  \textgreater{} By clicking Create an account, you agree to our
  \href{https://www.facebook.com/legal/terms}{Terms} \textgreater{} and
  that you have read our
  \href{https://www.facebook.com/about/privacy}{Data Policy}, including
  \textgreater{} our
  \href{https://www.facebook.com/policies/cookies/}{Cookie Use}.
  \textgreater{}
  \emph{{[}web\_2016\_facebooks-landing-page\_policy-acknowledgement{]}}
\item
  It can be seen more likely an information notice, that translates and
  specifies general given law, rather then a contract.
\item
  With such knowledge at hand, it is up to each individual, if the
  service's benefits are worth sharing some personal data, while
  simultaneously acquiescing potential downsides concerning the privacy
  of such data.
\item
  Every entity who is doing so, muss process Personal data according to
  the law and their \emph{Privacy Policy}. If they policies are
  violating existing law or the entity effectively goes against the law
  with their actual doing, penalties might follow. Depending on the
  level and impact of their infringement in addition the law itself,
  aside from revising their wrong-doings the entity might have to
  compensate the affected individuals, pay a fine or get revoked their
  license.
\item
  Not only privacy laws, but every legal jurisdiction has it's
  limitations - concerning their territorial nature - which makes
  legislation not exactly an appropriate tool when it comes to fixing
  existing issues and strengthen the individual's privacy and rights in
  a global context like the \emph{world wide web}. If no international
  agreement is in place
  {[}\protect\hyperlink{ref-web_2016_international-privacy-standards}{26}{]},
  only those laws are considered valid and enforcible where the
  organisation is registered, and maybe the fact where (meaning in which
  area of jurisdiction) the their servers are located or the data is
  processed and stored.
\end{itemize}

Whereas \textbf{Ownership} of \emph{Personal Data} has no legal ground
foundation what so ever. The concepts of intellectual property
protection and copyright might intuitively be applicable, because the
data, created by the data subject, seems to be her \emph{intellectual
property}. Such property implies to be a result of a creative process
though, but unfortunately there is no \emph{threshold of originality} in
facts, like \emph{personal information} is
{[}\protect\hyperlink{ref-paper_2014_who-owns-yours-data}{27}{]}.

\begin{itemize}
\item
  Ownership in the sense of having exclusive control over it's personal
  data and how they get processed at any given point in time; this not
  only comes with high costs, but is also very inconvenient for both
  parties - data subject and data consumer. It consists of two
  \protect\hypertarget{def-ownership}{}{concepts}: (A) the right to do
  what every is desired with their property and (B) in which rules and
  mechanisms the ownership can be assigned to someone
  {[}\protect\hyperlink{ref-book_1987_private-ownership_definition}{28}{]}.
\item
  The european DPR\footnote{EU Data Protection Regulation} contains only
  one occurrence of the word \emph{ownership}, which is not even related
  to the context of \emph{personal data} or the \emph{data subject}. It
  only stats, that \emph{``Natural persons should have control of their
  own personal data.''}
  {[}\protect\hyperlink{ref-regulation_2016_eu_general-data-protection-regulation_ownership}{29}{]}.
  Whereas Commissioner J. Rosenworcel of the FCC wants \emph{``consumers
  {[}\ldots{}{]} to {[}\ldots{}{]} take some ownership of what is done
  with their personal information.''}
  {[}\protect\hyperlink{ref-rules_2016_fcc_to-protect-broadband-consumer-privacy_ownership}{30}{]}
\item
  Typically the question of data ownership is addressed in data
  consumer's \emph{Terms of Service} (ToS), which an individual might
  have to accept in order to establish a (legal) relationship with it's
  author. I should be kept in mind, that \emph{ToS} might change over
  time; not necessarily to the users advantage. All addressed issues (by
  the ToS) must not violate any applicable or related law, otherwise the
  \emph{ToS} might not be legally recognized. Taking the following
  excerpts from different \emph{ToS}:
\end{itemize}

\begin{quote}
You own all of the content and information you post on Facebook, and you
can control how it is shared {[}\ldots{}{]}. \emph{(under ``2. Sharing
Your Content and Information'', by Facebook
{[}\protect\hyperlink{ref-web_2016_facebook_terms-of-service}{31}{]})}
\end{quote}

\begin{quote}
You retain your rights to any Content you submit, post or display on or
through the Services. What's yours is yours --- you own your Content.
\emph{(under ``3. Content on the Services'', by Twitter
{[}\protect\hyperlink{ref-web_2016_twitter_terms-of-service}{32}{]})}
\end{quote}

\begin{quote}
Some of our Services allow you to upload, submit, store, send or receive
content. You retain ownership of any intellectual property rights that
you hold in that content. In short, what belongs to you stays yours.
\emph{(under ``Your Content in our Services'', by Google
{[}\protect\hyperlink{ref-web_2016_google_terms-of-service}{33}{]})}
\end{quote}

\begin{quote}
Except for material we may license to you, Apple does not claim
ownership of the materials and/or Content you submit or make available
on the Service ``(under''H. Content Submitted or Made Available by You
on the Service``, by Apple
{[}\protect\hyperlink{ref-web_2016_apple-icloud_terms-of-service}{34}{]})*
\end{quote}

All these statements are followed by the same term, stating that the
user grants the author a worldwide license to do almost any imaginable
thing with her data. This even applies to Apple, if the user is
\emph{``submitting or posting {[}\ldots{}{]} Content on areas of the
Service that are accessible by the public or other users with whom
{[}the user{]} consent to share {[}\ldots{}{]} Content''}
{[}\protect\hyperlink{ref-web_2016_apple-icloud_terms-of-service}{34}{]}.

\begin{itemize}
\item
  It is worth noticing, that in every \emph{ToS} it is only referred to
  the data subject's content, not all her personal data. As mentioned
  above, personal information are no intellectual property, but playing
  an important role in data analytics though. Which is why \emph{privacy
  policies} are in place, to ensure at least some user enlightenment,
  even though it doesn't compensate the lack of control.
\item
  In addition to that, the meaning of \emph{ownership} used in the
  quoted \emph{ToS} is missing a clear outline and thus causing
  ambiguity and leaving room for interpretation. Nor the actual
  definition of \emph{ownership}, as described earlier, is applicable
  for these kind of cases, since the user losing all its control is by
  design. Handing over data to the consumer annihilates the exclusive
  control over the data and revokes the ability of assigning such
  control. There is no (legislation based) way to establish a feasible
  concept of \emph{ownership}, if the data consumer has no motivation to
  promote the user the a comprehensive owner of her data.
\item
  Leaving all the legal layer aside for a moment and switching the
  perspectives a bit; Data consumers might argue, that they had invested
  in enabling themselves to collect, process and store personal data, so
  it belongs to them. But from the data subject's point of view it might
  only be the case as long as as she would benefit as well somehow,
  e.g.~using products, services or features, offered by consumers, which
  quality depends on personal data. If the data subject chooses to move
  to a competitor might what to bring her personal data with her. But
  then again the former data consumer would object, competitors would
  benefit from all investments the consumer has made, but without any
  effort. Though, not entirely wrong, two aspects need to be emphasize.
  (A) In order to archive a high level of quality for their analytics
  and therefore in making right decisions to gain improvement, it's
  vital to huge amount of effort in developing these underlying
  technologies, not only in acquiring personal data. Which again only
  constitutes (B) the foundation of various subsequential computations
  followed by an ongoing collecting, aggregation and analytics of
  actively and passively created data and metadata (e.g.~food deliver
  history or platform interactions and tracking). Given the initially
  introduced definition of \emph{personal data} it appears to only be a
  fraction of the involved data belonging to its owner. The larger part
  consists of highly valuable metadata
  {[}\protect\hyperlink{ref-web_2013_why-metadata-matters}{35}{]}
  {[}\protect\hyperlink{ref-web_2016_why-you-need-metadata-for-big-data-to-success}{36}{]}
  and therefore should remain to the data collector and either be
  deleted or sufficiently anonymized, if the owner cancels the
  relationship. The data subject should not depend on the collector's
  willingness when it comes to handing over her personal data (e.g.~list
  of favorites or delivery history). Instead, using her own tool to
  provide the consumer with required data (e.g.~list of favorites) or
  tap into her data creating interactions (e.g.~food deliveries) on her
  own.
\item
  Whether an individual dies or a user deletes her account, as long as
  certain data point are shared with / connected to other users, the
  data will remain. At least when it comes to facebook.
\item
  Generally speaking, all data solely associating with an individual, is
  in the ownership of the same. But since it doesn't exist any legal
  concepts on \emph{personal data} ownership, a technical solution could
  help to regain some control.
\end{itemize}

\section{Personal Data in the context of the Big Data
Movement}\label{personal-data-in-the-context-of-the-big-data-movement}

\begin{itemize}
\item
  big data itself initially can be seen as a \emph{huge blob of data}
  containing more or less structured data sets
  {[}\protect\hyperlink{ref-web_2016_oxford_definition_big-data}{37}{]},
  whose size might have exceeded the capabilities of retrieving certain
  information almost only by hand. Such high data haystacks usually come
  along with new challenges in logistic and resource management, when
  information retrieval needs to get automated on a large scale
  {[}\protect\hyperlink{ref-web_2016_wikipedia_definition_big-data}{38}{]}.
  Theses practices are commonly referred to \emph{Big Data (Analysis)}
  including distributed computing and machine learning.
\item
  Big Data, or to be more precise, collecting and analyzing big data,
  serves the prior purpose to extract useful information, which on the
  other hand depends on what was the opening question about, but also
  what data sets the corpus is containing.
\item
  At first, (A) formalizing question(s) that the results have to answer.
  Secondly, (B) deciding what data is needed and appropriate and then
  start collecting. Third, (C) designing data models accordingly and
  correlate with the data (D) next, analyse and interpret the results.
  (E) last but not least, make business decisions based und the analyses
  ({[}\protect\hyperlink{ref-paper_2015_big-data-analytics_a-survey}{39}{]}
  Fig. 3).
\item
  machine learning/data mining --\textgreater{} computers trained to
  find coronations
\item
  since quite a few businesses (in terms of purpose or intention) are
  based around the concept of customers, which are generally somewhat
  entities consisting of at least one human being, personal data takes a
  major part in what \emph{Big Data} can be about. In the context of
  this thesis, these entities are individuals with a unique identity.
  And to understand the behaviour, decision making and needs of her
  customers a vendor, who owns the business, needs to know as much as
  possible about them, when she wants to know what changes she needs to
  address in order to move towards the most lucrative business.
\item
  personal data and information are reflecting all this knowledge. It
  starts with profile (or sensitive) data, such as gender, age,
  residency or income, goes on with time series events like geo-location
  changes, or web search history and goes all the way up to health data
  and self-created content like \emph{Tweets}\footnote{public massages
    published by an account on \url{twitter.com}, which will be
    displayed in the timeline of all her subscribers and also might
    contain additional types of content like images, links or video} or
  videos.
\item
  all these classes of personal data hold a major share\footnote{it
    doesn't matter whether an individual or just someone on behalf of an
    organisation spend money for something. at the end of the day, they
    are all humans on this planet and in a capitalistic oriented world
    money needs to flow and profits needs to be maximized. So to know
    where it will flow or why it will flow in a certain direction it is
    crucial to know everything about it's decision maker - the humans on
    this planet.} in the field of data analytics (TODO: find statistics
  showing shares of data types/classes/categories,
  {[}\protect\hyperlink{ref-book-chapter_1999_Principles-of-knowledge-discovery-in-databases_introduction-to-data-mining}{40}{]}
  {[}\protect\hyperlink{ref-web_2013_big-data-collection-collides-with-privacy-concerns}{41}{]})
\item
  but, depending on the specific attributes, they might be not that easy
  to acquire. in general most businesses obtain data from within their
  own platforms. some data might be in the user's rang of control
  (e.g.~customer or profile data), but most of the data comes from
  interacting directly (content creation, inputs) or indirectly
  (transactions, meta information). the level of sensitivity is mainly
  based on the purpose of the platform (benefit for the user) and what
  is the provider's demand from the users commitment (e.g.~required
  inputs or usage requires access to location)
\item
  from a technical perspective collecting passively created data is as
  simple as integrating logging mechanisms in the program logic. since
  the industry moved towards the cloud\footnote{side note - one might
    come to the conclusion, that only the trend towards the \emph{cloud}
    made it actually possible to collect to such an extent we are all
    observing these days, because standalone software should not
    necessarily require internet connection and therefore the vendors
    had no way to gather information whatsoever} most scenarios utilized
  server-client architectures. Furthermore the \emph{always-on}
  philosophy evolved to an imperative state. standalone software is
  starting to call the author's servers from time to time, just to make
  sure the user behaves properly. For browsers it was already a common
  narrative to make here and then requests to the server - still
  preventable though, but when it comes to native mobile apps it is
  almost impossible
  {[}\protect\hyperlink{ref-web_2016_answers-io}{42}{]} to notice such
  behaviour and therefore preventing apps from doing so.
\item
  these architectural developments were inducing the gathering of
  potentially useful information from all over the system on a large
  scale
  {[}\protect\hyperlink{ref-web_2016_big-data-enthusiasts-should-not-ignore}{43}{]}.
  Logging events, caused by the user's interactions, on the client,
  which then get forwarded to backend servers. Or keeping track of all
  kinds of transactions, which is done directly in the backend. Before
  running together in a designated place, all these collected chucks of
  data (TODO or ``data points'') are getting enriched with meta
  information. Finally get stored and probably never removed again - all
  for later analyses.
\item
  The mindset in the \emph{Big Data Community} is grounded on the basic
  assumption of \emph{more data is more helpful}, which already is
  emphasised by the often-cited concept of the three \emph{Vs} (Volume,
  Velocity, Variety)
  {[}\protect\hyperlink{ref-report_2001_3d-data-management-controlling-data-volume-velocity-and-variety}{44}{]}.
  which is not entirely wrong, because it lies in the nature of pattern
  and correlation discovery, to provide increasing quality results
  {[}\protect\hyperlink{ref-paper_2015_big-data-for-development-a-review-of-promises-and-challenges:more-data}{45}{]},
  while enriching the overall data with more precise data sets. But when
  new technologies are emerging, questioning the downsides and possible
  negative mid- or long-term impacts are typically not very likely to be
  a high priority. The focus lies on e.g.~trying to to reach and
  eventually breach boundaries while beginning to evolve. So
  non-technical aspects such as privacy and security awareness doesn't
  come in naturally, instead a wider range of research needs to be done
  alongside the evolution process and the increasing adoption rate in
  order to uncover such issues. Only then they can addressed properly on
  different levels - technical, political as well as social. So that the
  \emph{Big Data Community} itself is able to evolve, too. All in all
  it's a balancing act between respecting the user's privacy and having
  enough data at hand to satisfy the initial questioning with the
  computed results. Therefore people working in such contexts need to
  have advanced domain knowledge, be aware of any downsides or pitfalls
  and need to be sensible about the ramifications of their approaches
  and doings. Such improvements are already happening, not only
  originating from the field's forward thinkers
  {[}\protect\hyperlink{ref-web_2016_the-state-of-big-data}{46}{]}, but
  also advocated by governments, consumer rights organisations and even
  leading Tech-Companies start trying to do better
  {[}\protect\hyperlink{ref-web_2016_apple_customer-letter}{47}{]}
  {[}\protect\hyperlink{ref-web_2016_what-is-differential-privacy}{48}{]}
  {[}\protect\hyperlink{ref-web_2016_eff_whatsapp-rolls-out-emd-to-end-encryption}{49}{]}
  - as discussed in the section {[}TODO see personal data as of the
  law{]},
\item
  earlier in the text a difference was made between actively created and
  passively created data
\item
  based on that one could say \emph{profile/account data} is actively
  created, because it got into the system by the user's actively made
  decision to insert these information into a form and submit it - for
  whatever reason. whereas detecting the user's current location and
  adding this information to the submitted form is \emph{meta data}
\item
  of cause, it is debatable whether these kind of data belongs, in the
  sense of being the rightful owner, to the user or to the author or
  owner of the software containing the code that effectively created the
  data.
\item
  maybe personal data is every data/information whose creation (or
  digital existence) is a direct result of user interaction/engagement?
\item
  lets have a look into what the rule book says about that
  --\textgreater{} next topic (law)
\end{itemize}

\section{Personal Data as a Product}\label{personal-data-as-a-product}

\begin{itemize}
\item
  \emph{Big Data Analytics} by itself just comprises a structured and
  technical-aided procedure, serving the purpose of finding invisible
  information, that might be helpful to make (right) (business)
  decisions. Though, if one would ask data collectors about their
  motivation, most likely the answer would be something along the lines
  of PR phrasing like \emph{``We want to have a better understanding of
  our customers''}. But to do what exactly? To predict what might be the
  next thing I am supposed to buy Or what things I probably would like
  to consume but most certainly not yet know of?
\item
  Let's take a look at some examples. An advertising service uses
  tracking data for targeted advertising. The more information they have
  about an individual, the more accurate decisions they are able to make
  about what ads are the ones the individual most likely will click on
  and disclose with a successful purchase. As a result this makes the
  placed advertisement more valuable for ad service and therefore more
  expensive to the advertisers, because of a high precision. Or a
  streaming provider's content recommendation is also based on heavy
  user profiling done by looking at her consumption history, tracked
  platform interactions and probably many more vectors. Another example
  is \emph{Google Traffic}
  {[}\protect\hyperlink{ref-web_2007_introducing-google-traffic}{50}{]}
  {[}\protect\hyperlink{ref-web_2016_wikipedia_google-traffic}{51}{]}, a
  service, integrated as a feature in \emph{Google Maps}, which is
  Google's web mapping service. \emph{Google Traffic} visualises
  real-time traffic conditions, when using \emph{Maps} as a navigation
  assistant, to provide the user with a selection of possible paths, but
  enriched with duration, that takes such conditions into account. The
  data, required to offer these information, is supplied by mobile
  devices, constantly sending GPS coordinates with a timestamp into
  Google's infrastructure. This, however, only is made possible, because
  Google's services are widely used in addition to the fact that the
  majority of mobile devices
  {[}\protect\hyperlink{ref-graphic_2016_global-mobile-os-market-share}{52}{]}
  is driven by Android, an mobile operating system developed by Google,
  that deeply integrates with it's services. For this case the same
  assertion can be made - the more constantly streaming geo-location
  data, the more precise the information are about traffic conditions.
  Since this information demands the real-time aspect, adding time to
  the equation, add a other dimension of complexity to problem.
\item
  while the impact on our society of this first example group might be
  doubtable, a change of perspective opens up a different range of
  application areas. Such as

  \begin{itemize}
  \tightlist
  \item
    planing and managing human resources for situations, like e.g.~big
    events or emergency situations where attendees might need some help
    {[}\protect\hyperlink{ref-estimating-the-locations-of-emergency-events-from-twitter-streams_2014}{53}{]}
  \item
    predicting infrastructure workloads {[}TODO
    http://ieeexplore.ieee.org/document/7336197/{]}
  \item
    making more accurate diagnostics to improve their therapy
    {[}\protect\hyperlink{ref-the-practice-of-predictive-analytics-in-healthcare_2013}{54}{]}
  \item
    finding patters in climate changes, which otherwise wouldn't be
    detected
    {[}\protect\hyperlink{ref-data-collection-for-climate-changes_2014}{55}{]}.
  \end{itemize}
\item
  Through all these examples, some of them might not necessarily founded
  on personal data, whereas others primarily depend on them and yet
  others only implicitly rely on data collected from individuals. As
  always, it depends on the purpose - also known as \emph{business
  model} - but it seems to be consensual, that it all comes down to
  improving and enhancing the collector's product in order to satisfy
  the customers - and that on the other hand depends on what is meant to
  be the product and who is seen as customers.
\item
  Putting a top 10 list of industries using utilizing \emph{Big Data}
  {[}\protect\hyperlink{ref-graphic_2015_applications-of-big-data-in-10-industry-verticals}{56}{]}
  right next to visualization showing categories of personal data
  targeted by data collectors\\
  {[}\protect\hyperlink{ref-graphic_2012_personal-data-ecosystem}{57}{]},
  at least 7\footnote{Banking and Securities; Communication, Media \&
    Entertainment; Healthcare Providers; Government; Insurance; Retail
    \& Wholesale Trade; Energy \& Utilities} of these industries can be
  identified as data collectors, whereas less then a half\footnote{Banking
    and Securities; Communication, Media \& Entertainment; Insurance;
    Energy \& Utilities} are taking part of being a \emph{Data Broker},
  but almost all of them are using people's personal data, whether
  collected by themselves or acquired from \emph{Data Broker}.
\item
  At this point it's save to say, that \emph{Personal Data} is either
  seen directly as a product, especially from a Dater Broker's point of
  view, or indirectly due to it's essential part in \emph{Big Data}
  practices. The former generates direct revenue by selling these data
  and the latter might affect a business's product quality in a positive
  manner and thereby increasing revenue as well.
\item
  At the end it all comes down to understanding the human being and why
  she behaves as she does. The challenge is not only to compute certain
  motives but rather concluding to the right ones. When analyzing
  computed results with the corresponding data models and trying to
  conclude, it is important to keep in mind, that correlation is by far
  no proof of causation.
\item
  individuals then get in role of selling/offering it's own data to
  those who were previously collecting them
\end{itemize}

\section{Related Work}\label{related-work}

The idea of a digital vault, controlled and maintained by the data
subject, the individual, isn't that new. Holding her most sensitive and
valuable collections of bits and bytes, protected from all these data
brokers and authorities, while interacting with the digital and physical
world, opening and closing it's door from time to time, to either put
something important for her inside or retrieving an information
important for someone else. While in the mid and late 2000s the growth
of computer performance and capacity were crossing it's zenith (see
Moore's Law {[}\protect\hyperlink{ref-paper_1965_moors-law}{58}{]}), at
the same time the internet was starting to become a key part in many
people's lives and in society as a whole. Facilitated by these
circumstances, \emph{cloud computing} has been on the rise, causing the
shift towards parallel distributed processing and patterns alike.
Thereby making it possible to rethink solutions from the past and trying
to go new ways, namely the breakthrough 2007 in \emph{neuronal networks}
cutesy of G. Hinton
{[}\protect\hyperlink{ref-podcast_2015_cre-neuronale-netze}{59}{]}. As a
result, fields like \emph{deep machine learning}, \emph{big data
analytics} and most recently \emph{data mining}, were gaining a wide
range of attention. In almost any industry a greater amount of resources
is invested in these areas
{[}\protect\hyperlink{ref-web_2016_industries-intention-to-invest-in-big-data}{60}{]}.

The initial research motivation can be seen as a counter-movement away
from the \emph{cloud}, starting to focus again on privacy, the
individual and it's digital alter ego.

From simple middleware-solutions, via full-fledged software-based
platforms, through embedded hardware devices, a great variety of
approaches were starting to appear in the mid 2000s until this day. A
side effect was, that over time various research teams and projects have
invented and coined different terms, all referring to the same concept.
The following list shows some examples \emph{(alphabetical order)}:

\begin{itemize}
\tightlist
\item
  Databox
\item
  Identity Manager
\item
  Personal \ldots{}

  \begin{itemize}
  \tightlist
  \item
    Agent
  \item
    Container
  \item
    Data Store/Service/Stream (PDS)
  \item
    Data Vault
  \item
    Information Hub
  \item
    Information Management System (PIMS)
  \end{itemize}
\item
  Vendor Relationship Management (VRM)
\end{itemize}

One of the first research projects is \emph{ProjectVRM}, which
originated from \emph{Berkman Center for Internet \& Society} at
\emph{Harvard University}. As it's name implies, it was inspired by the
idea of turning the concepts of a \emph{Customer Relationship
Management} (CRM) upside down. This puts the vendor's customers back in
charge of their data priorly managed by the vendors. It also solves the
problem of unintended data redundancy. Over time the project has growing
to the largest and most influential in this research field. It
transformed into an umbrella and hub for all kinds of projects and
research related to that topic
{[}\protect\hyperlink{ref-web_2016_projectvrm_development-work}{61}{]},
whether it's frameworks or standards, services offering e.g.~privacy
protection, reference implementations, applications, software or
hardware components. \emph{VRM} became more and more a synonym for a set
of principles
{[}\protect\hyperlink{ref-web_2016_projectvrm_principles}{62}{]},
including for example \emph{``Customers must have control of data they
generate and gather. {[}They{]} must be able to assert their own terms
of engagement.''} These principles can be found in various ways across a
lot of research done within this area.

Another research that is worth mentioning, because of the foundational
work it has been done, is the european funded project called
\emph{Trusted Architecture for Securely Shared Service} (TAS3). The
project led to a open source reference implementation called
\emph{ZXID}.\footnote{more information on the project, the code and the
  author, Sampo Kellomäki, can be found under \emph{zxid.org}} The major
goal was, to develop an architecture, that takes all involved parties
into account, whether it's commercial businesses (vendors) or it's users
(customers), in order to fit into more sophisticated and dynamic
processes, but at the same time demanding a high level of user-centric
security facilitate i.a. by a developed policy framework. Due to these
requirements the architecture ended up being rather complex
{[}\protect\hyperlink{ref-graphic_2011_architecture_components-of-organization-domain}{63}{]}.
\emph{ZXID} as it's implementation incorporates several standards like
SAML 2.0\footnote{Security Assertion Markup Language 2.0} and
XACML,\footnote{eXtensible Access Control Markup Language} has only
three third-party dependencies which are \emph{OpenSSL}, \emph{cURL
(libcurl)} and \emph{zlib} and as of now it supports Java, PHP and Perl.
The project lasted for a period of 4 years, but after it ended in 2011,
the research work has pursued i.a. by the \emph{Liberty Alliance
Project}, which is now part of the \emph{Kantara Initiative}
{[}\protect\hyperlink{ref-web_kantara-initiative}{64}{]}, including all
documents and results. These results were taken up occasionally,
recently from the IEEE
{[}\protect\hyperlink{ref-paper_2014_personal-data-store-approach}{65}{]}.

A research project, which is probably the closest to what this document
aims to create, bears the name \emph{openPDS}
{[}\protect\hyperlink{ref-paper_2012_openpds_on-trusted-use-of-large-scale-personal-data}{66}{]}
and is done by \emph{Humans Dynamics Lab}
{[}\protect\hyperlink{ref-web_mit_openpds-safeanswers-project-page}{67}{]},
which is part of \emph{MIT Media Laboratories}. Despite the usual
concepts of a \emph{PDS}, it introduces multi-platform components and
user interfaces including a mobile devices and separating the
persistence layer physically at the same time. This facilitates
administrative tasks regardless of the data subject's position and time.
Moreover, with their idea of \emph{SafeAnswers}
{[}\protect\hyperlink{ref-paper_2014_openpds_protecting-privacy-of-meta-data-through-safeanswers}{68}{]},
the team even goes a step further. The concept behind that, is based
around \emph{remote code execution}, briefly described in
\protect\hyperlink{header-applying-for-a-loan-and-checking-creditworthiness}{one
of the user stories during the first chapter}. It abstracts the concept
of a data request to a more human-understandable level, a simple
question. This question consists of two representation: (A) a short
explanation of what the data consumer wants to know and which data might
be involved and thus what information a data consumer actually will
receive, instead of raw data the consumer could then use for all kinds
of purposes e.g. data aggregation or mining. Aside from that, the
request payload also includes (B) a code-based representation, which
gets executed in a sandbox on the data subjects's \emph{PDS} system with
the necessary data as arguments. The resulting output is answer and
response all in once.

Aside from all the research projects done within the scientific context,
applications with a commercial interest were starting to occur in a
variety of sectors, too. Microsoft's HealthVault
{[}\protect\hyperlink{ref-web_microsoft_healthvault}{69}{]}, for
example, which aims to replace all the paper-based patient file and
combine them in one digital version. This results in a patient-centered
medical data and documents archive, helping doctors to make the most
accurate decisions on medical treatment.

\emph{Meeco} {[}\protect\hyperlink{ref-web_meeco_how-it-works}{70}{]}
{[}\protect\hyperlink{ref-slides_2015_meeco-case-study}{71}{]}, based on
the MyData-Project
{[}whitepaper\_2014\_mydata-a-nordic-model-for-human-centered-personal-data-management-and-processing{]},
which essentially just cuts out the advertisement service provider as a
middle man inherits that role by itself. The platform does provide the
data subjects with more control over what information they reveal, but
it doesn't go the so eagerly demanded next step, which would means real
uncoupling from the advertisement market and finding a suitable business
model that focuses on the data subject, instead of surrounding them with
just another walled garden.

A recently announced project, sponsored by Germany's \emph{Federal
Ministry of Education and Research}, but developed and maintained
primarily by \emph{Fraunhofer-Gesellschaft} in cooperation with several
private companies like \emph{PricewaterhouseCoopers AG},
\emph{Volkswagen AG}, \emph{thyssenkrupp AG} or \emph{REWE Systems
GmbH}, is the so called \emph{Industrial Data Space}
{[}\protect\hyperlink{ref-web_industrial-data-space}{72}{]}. The project
unifies both, research and commercial interests and runs over time
period of three years until the third quarter of 2018. It aims to
\emph{``{[}\ldots{}{]} to facilitate the secure exchange and easy
linkage of data in business ecosystems''}, where at the same time
\emph{``{[}\ldots{}{]} ensuring digital sovereignty of data owners''}
{[}\protect\hyperlink{ref-whitepaper_2016_industrial-data-space}{73}{]}.
It will be interesting to see how these two, yet rather distinct
objectives, will come together in the future. Based on the white paper,
the project's focus mainly seems to lie in enabling and standardizing
the way companies collect, exchange and aggregate data with each other
across process chains to ensure high interoperability and accessibility.

Hereafter a selective list can be found of further research projects,
work and commercial products regarding the issue around \emph{personal
data}:

\textbf{Research}

\begin{itemize}
\tightlist
\item
  Higgins {[}https://www.eclipse.org/higgins/{]}
\item
  Hub-of-All-Things {[}http://hubofallthings.com/what-is-the-hat/{]}
\item
  ownyourinfo {[}http://www.ownyourinfo.com{]}
\item
  PAGORA {[}http://www.paoga.com{]}
\item
  PRIME/PrimeLife {[}https://www.prime-project.eu,
  http://primelife.ercim.eu/{]}
\item
  databox.me (reference implementation of the
  \emph{\href{https://github.com/solid/solid}{Solid framework}})
\item
  Polis (greek research project from 2008)
  {[}http://polis.ee.duth.gr/Polis/index.php{]}
\end{itemize}

\textbf{Organisations}

\begin{itemize}
\tightlist
\item
  Open Identity Exchange
  {[}http://openidentityexchange.org/resources/white-papers/{]}
\item
  Qiy Foundation {[}https://www.qiyfoundation.org/{]}
\end{itemize}

\textbf{Commercial Products}

\begin{itemize}
\tightlist
\item
  MyData {[}https://mydatafi.wordpress.com/{]}
\item
  RESPECT network {[}https://www.respectnetwork.com/{]}
\item
  aWise AEGIS {[}http://www.ewise.com/aegis{]}
\end{itemize}

\hypertarget{standards-specifications-and-related-technologies}{\section{Standards,
Specifications and related
Technologies}\label{standards-specifications-and-related-technologies}}

The overall attempt is to involve as much standards as possible, because
it increases the chances of interoperability and thereby it lowers the
effort, that might be needed, in order to integrate with third parties
or other APIs. Hereinafter, some of these possible technologies will be
touched on just briefly, why they might be a reasonable choice and what
purposes they might going to service.

\textbf{\protect\hypertarget{link_http}{}{HTTP}}
{[}\protect\hyperlink{ref-web_spec_http1}{74}{]}, well known as the
stateless \emph{``transport layer''} for the \emph{World Wide Web}, is
most likely going to fulfill the same purpose in the context of this
work, because it implements a server-client pattern in it very core.
Whether internal components (local or as part of a distributed system)
talk to each other or data consumers interact with the system, this
protocol transfers the data hat need to be exchanged. Features
introduced with Version 2
{[}\protect\hyperlink{ref-web_spec_http2}{75}{]} of the protocol are yet
to be known of their relevance of use cases within this project.
\emph{Websockes} {[}\protect\hyperlink{ref-web_spec_websockets}{76}{]}
might also be a possibility to communicate between components or even
with external parties, which has the advantage of high efficient ongoing
bidirectional connections using for real-time data exchange or remotely
pending process responses, while at the same time avoiding HTTP's
long-polling abilities.

\textbf{JSON}\footnote{The JavaScript Object Notation (JSON) Data
  Interchange Format; ECMA Standard\\
  {[}\protect\hyperlink{ref-web_spec_json}{77}{]} and Internet
  Engineering Task Force RFC 7159
  {[}\protect\hyperlink{ref-web_rfc_json}{78}{]}} is an alternative data
serialization format to XML, heavily used in web contexts to transfer
data via \emph{HTTP}, whose syntax is inspired by the JavaScript
object-literal notation.

The open standard \textbf{\protect\hypertarget{link_oauth}{}{OAuth}}
defines a process flow for authorizing third parties to access
externally hosted resources, such as the user's profile image from a
social media platform. The authorisation validation is done by the help
of a previously generated token. However, generating and supplying such
token can be initiated in a variety of ways depending on the already
existing architecture and design, e.g.~with the user entering her
credentials (\texttt{grant\_type=authorization\_code}). This design
tends
{[}\protect\hyperlink{ref-web_2012_problem-with-oauth-for-authentication}{79}{]}
to integrate \emph{OAuth} mistakenly but intentionally\\
as an authentication service rather then a authorization service;
regardless if as an alternative or as an addition to existing in-house
solutions. Therewith the application authors pass the responsibility on
to the OAuth-supporting data providers. While \emph{version 1.0a}
{[}\protect\hyperlink{ref-web_spec_oauth-1a}{80}{]}, commonly seen as a
protocol, provides integrity for transferred data by using signatures
and confidentiality by encrypting data ahead of transfer. \emph{Version
2.0} {[}\protect\hyperlink{ref-web_spec_oauth-2}{81}{]}, labeled as a
framework, on the other side requires \emph{TLS} and thus hands off the
the responsibility to confidentiality to the transport layer below. It
also includes certain process flows for specific platforms, such as
\emph{``web applications, desktop applications, mobile phones, and
living room devices''}
{[}\protect\hyperlink{ref-web_2016_oauth-2}{82}{]}.

With \textbf{OpenID} on the other side, the authenticity of a requesting
user gets verified, which is by design. An in-depth description of the
whole process can be found in the protocol's same-titled open standard.
With decentralisation kept in mind, the protocols's nature encourages to
design a distributed application architecture, similar to the idea
behind \emph{microservices}, but without owning all services involved,
\emph{decentralized authentication as a service} so to speak. An
application owner doesn't have to write or implement it's own user
management system, instead it is sufficient to just integrate these
parts from the standard need to support signing in with \emph{OpenID}.
Equally the user is not required to register a new account whenever it
is necessary, instead she can use her \emph{OpenID}, already created by
another identity provider, to authenticate with the application. The
extension \emph{OpenID Attribute Exchange} allows to import additional
profile data. \emph{OpenID Connect}
{[}\protect\hyperlink{ref-web_spec_openid-connect-1}{83}{]} is the third
iteration of the OpenID technology \emph{Connect} is to OpenID what
\emph{facebook connect} is to \emph{facebook}, except for the additional
authentication layer, which is build upon \emph{OAuth2.0} and therefore
enables, aside from authorisation mechanisms, third parties to
authenticate an OpenID-user and makes certain data available about that
account via REST interface.

If it's necessary for certain components, as part of a distributed
software, to make them stateless, apart from changing the architecture
so that the state at that point is not needed anymore, the only other
option would be to carry the state along (TODO: or ``passing the state
around''). This is a common use case for a
\textbf{\protect\hypertarget{link_jwt}{}{JSON Web Token}} \emph{(JWT)}
{[}\protect\hyperlink{ref-web_spec_json-web-token}{84}{]}. A \emph{JWT},
as it's name implies, is syntactically speaking formatted as
\emph{JSON}, but URI-safe into \emph{Base64} encoded, before it gets
transferred. The token itself holds the state. Here is where the use of
\emph{HTTP} comes in handy, because the token can be stored within the
HTTP header and therefore can be passed through all communication
points, where then certain data could be readout and therewith get
verified. Such a token typically consists of three parts: information
about itself, a payload, which can be arbitrary data such as user or
state information, and a signature; all separated with a period.
Additional standards define encryption \emph{(JWE\footnote{JSON Web
  Encryption, Internet Engineering Task Force RFC 7516
  {[}\protect\hyperlink{ref-web_spec_json-web-encryption}{85}{]}})} to
ensure confidentiality and signatures \emph{(JWS\footnote{JSON Web
  Signature, Internet Engineering Task Force RFC 7515
  {[}\protect\hyperlink{ref-web_spec_json-web-signature}{86}{]}})} to
preserve integrity of it's contents. Using a \emph{JWT} for
authentication purposes is described as \emph{stateless authentication},
because the verifying entity doesn't need to be aware of session IDs nor
any information about a state. So instead of the backend interface being
constrained to check a state (\texttt{isLoggedIn(sessionId)} or
\texttt{isAuthorized(sessionId)}) on every incoming request in order to
verify permissions, it just needs

When transferring data over a potential non-private channel several
properties might be desired, which eventually provide an overall trust
to that data. One important aspect might me, that no one else expect
sender and receiver are able to know and see what the actual data is. To
achieve this, \textbf{Symmetrical Cryptography} is used for. It states
that the sender encrypts the data with the help of a key and the
receiver decrypts that data also with that key. This is, sender and
receiver, both need to know that one key, but everyone else should not .
To agree on a key without compromising the key during that process, both
entities either change the medium (e.g meet physically and exchange) or
have to use a procedure, in which at any point in time the entire key is
not exposed to others then sender and receiver. This procedure is called
\textbf{Diffie-Hellman-Key-Exchange}
{[}\protect\hyperlink{ref-paper_1976_d-h-key-exchange}{87}{]} and is
based on rules for modulo operations when prime numbers are involved. It
is designed with the goal to agree on a \emph{secret} while at the same
time using a non-private channel. The data exchanged during the process
alone can't be used to deduce the secret. Such behaviour is similar to
the concepts of
\textbf{\protect\hypertarget{link_asym-crypto}{}{Asymmetrical
Cryptography}} \emph{(or public-key cryptography)}
{[}\protect\hyperlink{ref-book_2014_chapter-9-1-public-key-crypto}{88}{]},
which is underpinned by a \emph{key-pair}; one key is \emph{public} and
the other one is \emph{private}. Depending on which of keys is used to
\emph{encrypt} the data, only the other one can be used for
\emph{decrypting} the cipher. If then this technology gets combined with
the concept of digital signatures (encrypted fingerprints from data),
together it would provide integrity and authentication.

Wrapping \emph{HTTP} in the \emph{Transport Layer Security}
{[}\protect\hyperlink{ref-web_spec_tls}{89}{]} (\emph{TLS}) results in
\textbf{HTTPS}. TLS provides encryption during the data transport, which
reduces the vulnerability to \emph{man-in-the-middle} attacks and thus
ensures not only confidentiality but data integrity as well.
\emph{Asymmetric cryptography} is the foundation for the connection
establishment, hence \emph{TLS} also allows to verify integrity of the
entity on the the connection's counterside, and, depending on the
integration, it could even be used for authentication purposes. But
relying on those cryptographical concepts requires additional
infrastructure. Such an infrastructure is known as \emph{Public Key
Infrastructure} (or \emph{PKI})
{[}\protect\hyperlink{ref-book_2014_chapter-14-5-pki}{90}{]}. It manages
and provides public keys in a directory, including related information
to the owners of these certificates. A Certificate Authority (or
\emph{CA}), as part of that infrastructure, issues, maintains and
revokes digital certificates. The infrastructure that is needed to
provide secure HTTP connections for the internet is one of those
\emph{PKI}s - a public one and probably the largest. It is based on the
widely used IETF\footnote{Internet Engineering Task Force; non-profit
  organisation that develops and releases standards mainly related to
  the Internet protocol suite} standard \emph{X.509}
{[}\protect\hyperlink{ref-web_spec_x509}{91}{]}.

\textbf{REST(ful)}\footnote{\emph{Representational State Transfer},
  introduces by Roy Fielding in his doctoral dissertation
  {[}\protect\hyperlink{ref-web_spec_rest}{92}{]}} is a common set of
principles to design web resources communication, primarily
server-client relations, in a more generic and thereby interoperable
way. Aside from hierarchically structured URIs, which reflect semantic
meanings, it involves a group of rudimentary vocabulary\footnote{knows
  as HTTP Methods or Verbs
  {[}\protect\hyperlink{ref-web_spec_http-methods}{93}{]} (e.g.~GET,
  OPTIONS, PUT, DELETE)} to provide basic Create-Read-Update-Delete
operations across distributed systems. The entire request need to
contain everything that is required to get proceeded, e.g.~state data
and possibly authentication. These operation normally wont get applied
directly to the responsible component. Instead the whole system (or
certain services) exposes a restful API, with which a third party can
then interact.

The \emph{QL} in \textbf{GraphQL}
{[}\protect\hyperlink{ref-web_spec_graphql}{94}{]} stands for
\emph{query language}. It's goal is to abstract multiple data sources in
order to unify them under one API and make all containing data
queryable, including all relating data points. The returned data,
emitted in JSON syntax, can exhibit graph-like structures, meaning
multiple data points, that might be somehow related to each other, or in
other words: indirectly ``linked'' through each other. These, naturally
deep-leveled structures, can be described by the syntax of the query
language.

The term \textbf{Semantic Web} bundles a conglomerate of standards
addressing syntax, schemas, assess control and integration around the
idea of \emph{web of data} to \emph{``allow data being shared and reused
across''} {[}web\_2016\_w3c\_semantic-web-activity{]} or within several
scopes and contexts. Alongside several others, the following three
standards have a certain relevance to that concept. RDF\footnote{Resource
  Description Framework {[}\protect\hyperlink{ref-web_w3c-tr_rdf}{95}{]}}
basically defines the syntax. OWL\footnote{Web Ontology Language
  {[}\protect\hyperlink{ref-web_w3c-tr_owl}{96}{]}} provides the
guidelines on how the semantics and schemas should be defined and with
SPARQL {[}\protect\hyperlink{ref-web_w3c-tr_sparql}{97}{]}, the query
language for the RDF format, the data can be retrieved. A picture
emerges in which the web is used as a database, queried by URIs with a
query language. An example would be a person's email address, which is
available under a specific domain (preferable owned by that person) - or
to be more precise, an URI \emph{(WebID)
{[}\protect\hyperlink{ref-web_w3c-draft_webid}{98}{]}} - and provided in
a certain syntax \emph{(RDF)} and tagged with the semantic \emph{(OWL)}
of a email address; all embedded in a valid html page. This information
can be queried \emph{(SPARQL)}, which requires at least the URI, working
as a unique identifier. While defining the standards, an importancy was
to define a syntax which is also valid markup, in order to maintain a
single source of trough and save redundant work. Related to this topic
is the work on a specification called \textbf{Solid}.\footnote{social
  linked data {[}\protect\hyperlink{ref-web_spec_solid}{99}{]}} Based on
the \emph{Linked Data} principles, that are facilitated through the
standards just mentioned and the \emph{WebAccessControl}
{[}\protect\hyperlink{ref-web_2016_wiki_webaccesscontrol}{100}{]}
system, the project focuses on decentralization and personal data. A
reference implementation called \emph{databox}
{[}\protect\hyperlink{ref-web_2016_demo_databox}{101}{]} combines all
these technologies and is build on top.

The concept of application (or software) \textbf{container} is about
encapsulating runtime environments by introducing an additional layer of
abstraction. A container bundles just the software dependencies
(e.g.~binaries) that are absolutely necessary so that the enclosed
program is able to run properly. The actual container separation is
done, aside from others, with the help of two features provided by the
Linux kernel. \emph{Cgroups},\footnote{control groups
  {[}\protect\hyperlink{ref-web_2015_cgroup-doc}{102}{]}} which define
or restrict how much of the existing resources a group of processes
(e.g.~CPU, memory or network) can use. Whereas \emph{namespaces}
{[}\protect\hyperlink{ref-web_2016_kernel-namespace}{103}{]} define or
restrict what parts of the system can be accessed or seen by a process
(e.g.~filesystem, user, other processes). The idea of encapsulating
programs from the operating system-level is not new, Technologies, such
as \emph{libvirt}, \emph{systemd-nspawn}, \emph{jails}, or
\emph{hypervisors} (e.g.~VMware, KVM, virtualbox) have been used for
years, but were usually too cumbersome and never reached a great level
of convenience, so that only people with a certain expertise were able
to handle systems build upon virtualization, but people with other
backgrounds couldn't and weren't that much interested. Until
\emph{Docker} and \emph{rkt} emerged. After some years of separated
work, both authors, and others, recently joined forces in the \emph{Open
Container Initiative}
{[}\protect\hyperlink{ref-web_2016_open-container-initiative}{104}{]},
which aims to harmonize the diverged landscape and start building common
ground to ensure a higher interoperability, and that in turn is
requisite for orchestration. It also marks the initial draft of the
specifications for runtime
{[}\protect\hyperlink{ref-web_oci-spec_runtime}{105}{]} and image
{[}\protect\hyperlink{ref-web_oci-spec_image}{106}{]} definition, on
which the work is still ongoing. This concept of \emph{containerization}
also inherits the a ability known from \emph{emulation}, because it
allows a certain set of software to run on a system that otherwise is
not supported, e.g.~mobile devices. It only requires the runtime to be
working.

In the past years different countries around the world started to
introduce \emph{information technology} to the day-to-day processes,
interactions and communications between public services and their
citizens, for example changing residence information or filing tax
report, which is summarized under the term
\emph{E-government}.\footnote{Electronic government} One of those
developments is the so called \textbf{Electronic ID
Card}\{\#link\_eid-card\}, hereinafter called \emph{eID card}. Equipped
with storage, logic and interfaces for wireless communication, those
\emph{eID cards} can be used to store certain information and digital
keys or to authenticate the owner electronically to a third party
without being physically present. Such an \emph{eID card} was also
introduced in Germany in 2010. The so called \emph{nPA}\footnote{in
  german so called \emph{elektronische Personalausweis (nPA)}} was an
important step towards an operational \emph{e-government}. Aside from
minor flaws
{[}\protect\hyperlink{ref-web_2013_npa-sicherheitsdefizit}{107}{]} and
disadvantages
{[}\protect\hyperlink{ref-web_2014_test-qes-support-in-npa}{108}{]} an
\emph{eID card} can come with, the question here is, how can such
technology be usefully integrated in this project and does it even makes
sense. As an official document the card has one major advantage over
self-configured or generated authentication mechanisms like passwords,
fingerprints or TANs.\footnote{Transaction authentication number} It is
\emph{signed} by design, which means, by creating this document and
handing it over to the related citizen, the third party (or
\emph{``authority''}) - in this case the government - has verified the
authenticity of that individual.

When communication via email it is already common to encrypted the
transport channel, but using \emph{asymmetric cryptography} for
encrypting emails end-to-end is rather unusual. The equivalent to a
\emph{PKI} would be basically a \emph{public key server} that follows a
concept called \emph{web of trust}. In which all entities (user; senders
and recipients) are signing each other's public keys. The more users
have signed a public key, the higher the trust that this key actually
belongs to the user that it says it does. That public key is then simply
uploaded by the owner to public servers where another user, who wants to
write that key owner an email can obtain keys. Related to that topic,
another technology emerging as part of the \emph{e-government}
development, is the german \textbf{De-Mail}
{[}\protect\hyperlink{ref-web_2017_about-de-mail}{109}{]}. It's an
eMail-Service that is meant to provide infrastructure and mechanisms to
exchange legally binding electronic documents. One would expect a
\emph{public key cryptography}-based implementation all the way from
sender through to the recipient
{[}\protect\hyperlink{ref-statement_2013_de-mail}{110}{]}, maybe even
with taking advantage of the \emph{nPA's} capability to create
\emph{QES}, which refers to the ability of using the \emph{nPA} to sign
arbitrary data. Instead, the creators of the corresponding law decided
that it's enough to prove the author's identity if the provider signs
the document on the email server and that this implementation results in
a legally binding document by definition of that law.

\hypertarget{core-principles}{\chapter{Core
Principles}\label{core-principles}}

Right from the start a set of principles have build the cornerstones and
orientation marks of the idea behind the \emph{PDaaS}. Those, who meant
to be reflected also by the arising \emph{Open Specification}, will be
explained further within the following sections.

\section{Data Ownership}\label{data-ownership}

Depending on the standpoint, the question about ownership of certain
data might not that trivial to answer. As stated in the
\protect\hyperlink{digital-identity-personal-data-and-ownership}{previous
section}, ownership requires a certain amount of originality to become
intellectual property, which is not the case for personal data - at
least for all the non-creative content. Thus there is no legal ground
for an individual to license those data, that obviously belongs to her.
Switching the perspective from the \emph{data subject} to the data
\emph{consumer}; for them, several laws exist addressing conditions and
rules regarding data acquisition, processing and usage. Leaving aside
the absence of any legislation regarding data ownership, it cannot be
denied, that is seems unnatural not being the owner of all the data that
reflects her identity and her as an individual. So instead of defining
those rules meant to protect data subjects, but demanding data consumers
to comply with, the proposal here is to put the entity, to whom the data
is related to, in control of defining, who can access her data and what
accessor is allowed to do with it. This would make the \emph{data
subject}, \protect\hyperlink{def-ownership}{per definition} and
effectively to the owner of those data. Although, it is to be noted,
that the legal rulebook for data consumers mentioned before, remains a
highly important, since this project is not able to cover every use
case, that might occur.

Promoted from the data subject to the data owner, hence being the center
of the \emph{PDaaS}, the operator gains abilities to have as much
control as possible over all the data related to her, to determine in a
very precise way what data of hers can be accessed by third parties at
any point in time and to literally carry all her personal data with her.

\section{Identity Verification}\label{identity-verification}

When an instance of this system is going to be the digital counterpart
of an individuals identity or it's \emph{personal agent}
{[}\protect\hyperlink{ref-book_2015_ethical-it-innovation}{111}{]}, then
everyone who relies on the information that agent is providing, must as
well be able to trust the source from where that data is coming from and
vice versa; the \emph{operator} too must be able to verify the
authenticity of the requesting source; regardless if it's the initial
\emph{permission request} or further \emph{access attempts}. Based on
these mechanisms, the system can also provide an authentication services
to all sorts of generic or restricted platforms for the associated
identity, including second factor abilities.

\section{Reliable Data}\label{reliable-data}

Being able to verify the authenticity of a communication partner means
only to be half-way through. Data consumers also need to trust the data
itself, which is attributed to the following properties.

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  \emph{integrity} - which means the recipient can verify, that the
  data, sent to her, is still the same, or if someone has tampered with
  the obtained data. (B) \emph{authenticity} - it is somehow ensured, or
  the recipient must be certain, that the received data belongs to the
  individual from whom the data comes from. A negative result of that
  check should not cause a termination of the process, but instead
  should warn the recipient about the lack of authenticity, so that she,
  herself, can decide if and how to proceed.
\end{enumerate}

\section{Authorisation}\label{authorisation}

Controlling it's own data might probably be the most important ability
of such a system, because the data owner gets enabled to grant
permission to any entity who want to obtain certain information about
her in a semi-automated way. She can authorise as precise as desired how
long and what data (sets, points or fields) is accessible by a single
entity. Thereby, the data owner is able to change the \emph{access
permissions} for any entity at any point in time, for example motivated
by a noticed incident.

\section{Supervised Data Access}\label{supervised-data-access}

Rules and constraints might be one way to handle \emph{personal data}
demands of \emph{third parties}. But this plain \emph{query and response
data} approach could be replaced by a more supervised concept, that
prevents data from leaving the system. It allows to execute a small
program within a locally defined environment, computing only a fraction
of a larger computation that was initiated by the \emph{data consumer}
beforehand; similar to a distributed Map-Reduce concept
{[}\protect\hyperlink{ref-paper_2004_distributed-mapreduce}{112}{]}. The
opposite approach, to provide some software to the \emph{data consumer}
that is necessary to access the contents of a response or provides a
runtime environment querying the system by itself, would be conceivable
as well. In general, it is not very likely that \emph{data consumers},
who already got granted certain access, would renounce their privileges.
Thus it is vital that the \emph{data owner} is the one who is able of
cancel the \emph{access permissions} or applying appropriate changes.
Supervising methods provide an appropriate ways to make data available
to those who are eager to consume them.

\section{Containerization}\label{containerization}

Abstracting an operating system by moving the bare minimum of required
parts into a virtualization results into an environment that can be,
depending on the configuration, fully encapsulate it's internals from
the host environment. This approach yields to some valuable features.
Such as:

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  Effortless portability, which reduces the requirements on environment
  and hardware to a minimum.
\item
  Thereby gaining higher flexibility in placing components, through
  which advantages can be made out of other devices characteristics.
  while not necessarily increasing the overall complexity of the system
\item
  Isolation and reduction of shared spaces and scopes, which for example
  can prevent side effects.\\
  All these in conjunction lead also to an overall security improvement
  or at least it enables new patterns to improve such aspects.
  Furthermore, it allows to suit more versatile and diverse scenarios,
  like storing data about a using data, providing sensitive profile data
  or getting used as a patient file. The convenience of a precise
  resource assignment might also become relevant for case where device's
  hardware specification might be somewhat low. Building a system upon a
  container-based philosophy and enclosing components in their own
  environment brings a variety of design and architectural possibilities
  without the necessity of increasing the overall system complexity.
\end{enumerate}

\section{Open Development}\label{open-development}

When developing an \emph{Open Specification} it only comes natural to
build upon open technologies, which are understand as \emph{open
standards} and \emph{open source}; \emph{open} in the sense of
\emph{unrestricted accessible by everybody} and not to be confused with
free - as in \emph{freedom} - software. Advocating such a philosophy
permits not only to develop implementations in a collaborative way, but
enables\\
also to work fully transparent on the specification itself. Such an open
environment makes it possible for anyone who is interested, to
participate or even to contribute to the project. Thus, to lower the
barrier, usable and meaningful documentation is vital. Such an openness
ensures the possibility of looking into the source code and getting a
picture of what the program actually does and how it works. Thus, source
code reviews become possible as well. Those might reveal certain
security flaws, which then are able to get fixed very quickly.
Furthermore, this approach allows data subjects to setup their own
infrastructure and host such a system, which gains even more control
over the data and increases the level of trust, instead of using a
\emph{SaaS}\footnote{Software as a Service} solution that is host by
another provider. It also encourages any kind of adjustments or
customization to the system in order to serve the own's needs. Enabling
an open development allows users and contributors working together and
thus improve the outcome in a variety of ways.

\hypertarget{requirements}{\chapter{Requirements}\label{requirements}}

Derived from the \protect\hyperlink{core-principles}{Core Principles},
the subsequent requirements shall be served as a list of features on the
one hand, to get an idea about how the open specification and thus the
resulting software might look like, and to give an overview about
priorities (can/could, may/might, should, must/have to) on the other
hand. Other chapters may contain specific references to the requirements
listed below.

\subsubsection{Architecture/Design:}\label{architecturedesign}

\textbf{\emph{\protect\hypertarget{sa01}{}{S.A.01}} - Accessibility \&
Compatibility}\\
Since the internet is one of the most widely used infrastructure for
data transfer and communication, it is assumed that all common platforms
support underlying technologies, such as HTTP and TLS. Thus the emerging
system should implement a web service, who provides supervised access to
personal data.

\textbf{\emph{\protect\hypertarget{sa02}{}{S.A.02}} - Portability}\\
All major components should be designed and communicate between each
other in a way to be able to get relocated while the system has to
remain fully functional. It has to be possible to build a distributed
system, that may require to place certain components into different
environments/devices.

\textbf{\emph{\protect\hypertarget{sa03}{}{S.A.03}} - Roles}\\
The system has to define two types of roles. The first one is the
\protect\hyperlink{terminologies--operator}{operator}, who is in control
of the system and, depending on the architecture, must be at least on
individual but can be more. The operator takes care of all the data that
then get's provided and decides about which third party get's access to
what data. The second type are the
\protect\hyperlink{terminologies--consumer}{consumers}. These are
external third parties that desire certain data about or from the
operator. (see \protect\hyperlink{terminologies}{Terminologies})

\textbf{\emph{\protect\hypertarget{sa04}{}{S.A.04}} - Authenticity}\\
Since they have to rely on the data, both entities - everyone who
belongs to one of the \emph{\protect\hyperlink{sa03}{roles}} - have to
be able to ensure the authenticity of their identity and the data they
are sending to the opponent. It should be possible to opt out to that
level of reliability, if is not necessary, or to opt-in for certain
aspects. However, if one of the parties demanding the other one of
providing such level, but the other doesn't, then the access attempt has
to fail.

\subsubsection{Persistence:}\label{persistence}

\textbf{\emph{\protect\hypertarget{sp01}{}{S.P.01}} - Data Outflow}\\
Data may only leave the system if it's absolutely necessary and no other
option exists to preserve the goal of that process. But if data still
has to get transferred, no other than the data consumer must be able to
access the data. Confidentiality has to be preserved at all cost.

\textbf{\emph{\protect\hypertarget{sp02}{}{S.P.02}} - Data
Relationship}\\
Data structures and data models must show high flexibility and may not
consist of strong relations and serration.

\textbf{\emph{\protect\hypertarget{sp03}{}{S.P.03}} - Schema and
Structure}\\
The \emph{Operator} can create new data types (based on a schema) in
order to extend the capabilities of the data API. Structures and schemas
can change over time (\protect\hyperlink{sp04}{S.P.04}). Every data set
and data point has to relate to a corresponding and existing type,
whether it's a simple type (string, integer, boolean, etc.) or a
structured composition based on a schema.

\textbf{\emph{\protect\hypertarget{sp04}{}{S.P.04}} - Write}\\
Primarily the operator is the only one who has the permissions to add,
change or remove data. This is done either by using the appropriate
forms provided by visual user interface or import mechanisms. The latter
could be enabled through (A) support for file upload containing
supported formats, (B) data API restricted to the operator or (C)
defining an external source reachable via http (e.g. \emph{RESTful URI})
in order to (semi-)automate additional an ongoing data import from
multiple data sources (e.g.~IoT, browser plugin). Additionally, it might
be possible in the future to allow \emph{data consumers} letting some
data to flow back into the operator's system, after she is certain about
it's validity and usefulness.

\subsubsection{Interfaces:}\label{interfaces}

\textbf{\emph{\protect\hypertarget{si01}{}{S.I.01}} - Documentation}\\
The interfaces of all components have to be documented; in a way that
the components themselves can be replaced without any impact to the rest
of the system. This also involves comprehensive information on how to
communicate and what endpoints are provided, including required
arguments and result structure.

\textbf{\emph{\protect\hypertarget{si02}{}{S.I.02}} - External Data
Query}\\
Data consumer can request a schema, in order to know how the response
data will actually look like, since certain parts of the data structure
might change over time (see \protect\hyperlink{sp03}{S.P.03},
\protect\hyperlink{sp04}{S.P.04}). After checking if the access request
is permitted, the system first parses and validates the query and
eventually proceeds to actually execute the included query. When
querying data from the system, the \emph{data consumer} might be
required tp provide a schema, which should force him to be as precise as
possible about what data is exactly needed. In addition to that, the
consuming entity must provide some \emph{meaningful} text, describing
the purpose of the requested data. He should not be allowed to place
wildcard selectors for data points in the query. Instead he must always
define a more specific filter or a maximum number of items, if the query
retrieves more then one element.

\textbf{\emph{\protect\hypertarget{si03}{}{S.I.03}} - Formats}\\
When components communicating between each other or interactions with
the system from the outside take place, all data send back and forth
should be serialized/structured in a JSON or JSON-like structure.

\subsubsection{Visual User Interface:}\label{visual-user-interface}

\textbf{\emph{\protect\hypertarget{pviu01}{}{P.VIU.01}} - Responsive
user interface}\\
The visual user interface has to be responsive to the available space,
because of the diversity of screen sizes nowadays.

\textbf{\emph{\protect\hypertarget{pviu02}{}{P.VIU.02}} - Platform
support}\\
The user interface must be at least implemented based on web
technologies, that is provided by a server and is thus available on any
platform that comes with a modern browser. To enable additional features
and behavior, at least for mobile devices it is recommended to build a
user interface upon native supported technologies, such as \emph{Swift}
and \emph{Java}. The operator would benefit from capabilities such as
\emph{push notifications} and storing data on that device.

\textbf{\emph{\protect\hypertarget{pviu03}{}{P.VIU.03}} - Permission
Profiles}\\
The operator should be capable of filtering, sorting and searching
through the list of \emph{access profiles}; for a better administration
experience and to easily find certain entries while the overall amount
increases over time.

\textbf{\emph{\protect\hypertarget{pviu04}{}{P.VIU.04}} - Access
History}~ The operator must be provided with a list of all past
permission requests and data accesses, in order to monitor who is
accessing what data and when, and thus being capable of evaluating and
eventually stopping certain access and data usage. This tool should have
filter, search and sort capabilities. It is build upon and therefore
requires the \protect\hyperlink{pb01}{access logging} functionality.

\subsubsection{Interactions:}\label{interactions}

\textbf{\emph{\protect\hypertarget{pi01}{}{P.I.01}} - Effort}\\
Common interactions processes, like changing \emph{profile data},
importing data sets or manage \emph{permission request} have to require
as little effort as possible. This means short UI response time on the
one hand and as less single input and interaction steps as possible to
complete a task. Given these circumstances, the \emph{permission request
review} and \emph{permission profile creation} might become a special
challenge.

\textbf{\emph{\protect\hypertarget{pi02}{}{P.I.02}} - Design}\\
The visual user interface must be designed and structured in such a way
that is is highly intuitive for the user to operate. Thus, it is
important e.g.~to use meaningful icons and appropriate labels. It also
means a flat and not crammed menu navigation. Context related
interaction elements should be positioned within the area designated for
that context. TODO: maybe emphasize more UI aspects (or not)

\textbf{\emph{\protect\hypertarget{pi03}{}{P.I.03}} - Notifications}\\
The user should be notified about every interaction with the
\emph{PDaaS} originated by a third party immediately after it's
occurrence, but she must get notified at least about every
\emph{permission request}. This behaviour should be configurable;
depending on the \emph{permission type} and on every \emph{permission
profile}. Regardless of the configuration the notifications themselves
must show up and pending user interactions must be indicated in the user
interface.

\textbf{\emph{\protect\hypertarget{pi04}{}{P.I.04}} - Permission Request
\& Review}\\
A process involving data transaction must always be initiated by the
data subjects. So before a \emph{data consumer} is able to access data,
first the \emph{operator} need to \emph{invite} him and tell him whereto
address his requests. This has to be done by sending him a URI leading
to an endpoint, that needs to be unique among all \emph{data consumers}
interacting with the same instance of the system. When a \emph{data
consumer} makes the first attempt to connect to the system, it must be a
well formed \emph{permission request}, which has to include information
about the \emph{consumer}, what data he wants to get access to, for what
purpose and how log or how often the data need to be requested. The
operator then reviews these information and creates an \emph{permission
profile} based on that information. A key configuration in such a
profile has to be what defines when this permission expires. The
operator should be able to decide between three \emph{permission types}:

\begin{itemize}
\tightlist
\item
  \emph{one-time-only}
\item
  \emph{expires-on-date}
\item
  \emph{until-further-notice} After creating the profile, a response
  must be send to the \emph{data consumer}, which should contain the
  review result and permission type set by the operator.
\end{itemize}

\textbf{\emph{\protect\hypertarget{pi05}{}{P.I.05}} - Templating}\\
The operator should be able to create templates for \emph{permission
profiles} nad \emph{permission rules} in order to (A) apply a set of
configuration in advance before the \emph{permission request} arrives
and

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  reduce recurring redundant configurations.
\end{enumerate}

\subsubsection{Behaviour:}\label{behaviour}

\textbf{\emph{\protect\hypertarget{pb01}{}{P.B.01}} - Access Logging}\\
All interactions and changes in the persistence layer should be logged.
At least all data request must be logged. Such log is the foundation of
the \emph{access history}, with this the user is able to keep track of
and look up past accesses.

\textbf{\emph{\protect\hypertarget{pb02}{}{P.B.02}} - Real time}\\
Real time communication might be essential for time-critical data
transaction. Hence at least one user interfaces should be connected to
the server through an ongoing connection to enable real time support
(example scenario: permission request got reviewed on mobile device, but
notification indicator reflects ``still pending''). But if just one
client is associated to the system, real time (in the sense of keeping
UI state up to date) would not be necessary. (see
\protect\hyperlink{pviu02ux5cux257D}{P.VIU.02})

\chapter{Design Discussion}\label{design-discussion}

The following chapter documents the processes of some design decision
makings, examines possible issues emerging alongside and discuses
different solutions obtained from several perspectives in order to
evaluate their advantages and disadvantages. Probably not every issue
will get it's\\
deserved room, but major aspects will be addressed. In short, the
majority of the project's conceptual work is done below.

Eevery subchapter includes at the end a section containing a summary of
conclusions, which are based on the prior discussions about the related
topic.

\hypertarget{authentication}{\section{Authentication}\label{authentication}}

First of all, the system has to support two
\protect\hyperlink{sa03}{roles}. Any entity can be assigned to either
one of them, hence entities that are trying to authenticate to the
system might have different intentions. The \emph{operator} for example
wants to review \emph{permission requests} in real time, so accessing
the system from different devices is a common scenario. When inheriting
the \emph{operator role} an entity gains further capabilities to
interact with the system, such as data manipulation. Whereas a
\emph{data consumer} always uses just one origin and processes requests
sequentially. Those very distinct groups of scenarios would make it
possible to apply different authentication mechanisms that do not
necessarily have a lot in common.

With respect to the requirements (\protect\hyperlink{sa01}{S.A.01}), the
most appropriate way to communicate with the \emph{PDaaS} over the
internet would be by using \emph{\protect\hyperlink{link_http}{HTTP}}.
Furthermore, to preserve confidentiality on every in- and outgoing data
(\protect\hyperlink{sp01}{S.P.01}) the most convenient solution is to
use \emph{HTTP} on top of \emph{TLS}. \emph{TLS} relies i.a. on
\protect\hyperlink{link_asym-crypto}{asymmetric cryptography}. During
the connection establishment the initial handshake requires a
certificate, issued and signed by a CA, which has to be provided by the
server. This ensures at the same time a seasonable level of identity
authentication, almost effortless. If the certificate is not installed,
it can be installed manually on the client. If the certificate is not
trusted (e.g.~it is self-signed), it can either be ignored or the
process fails to establish a connection, depending on the server
configurations. The identity verification in TLS works in both
directions, which means not only the client has to verify the server's
identity by checking the certificate. If the server insists on, the
client has to provide a certificate as well, which then the server tries
to verify. Only if the outcome is positive, the connection establishing
succeeds. According to the specification
{[}\protect\hyperlink{ref-web_spec_tls-12_client-auth}{113}{]} it is
still optional though.

\emph{HTTP} as a comprehensive and flexible protocol enables to use
several technologies for server-client authentication purposes. Some of
them are build-in, others can simply be implemented on top of the
protocol. Within the scope of this work, those technologies are
categorized in the following types (TODO: maybe find other labels): (A)
stateful and (B) stateless authentication. The first one (A) includes
vor example \emph{Basic access Authentication} (or \emph{Basic Auth})
and authentication based on \emph{Cookies}. Whereas the \emph{two-way
authentication} in TLS mentioned above and
\protect\hyperlink{link_jwt}{authentication based on web-token} are
associated with the latter (B). \emph{Basic Auth} is natively provided
by the \emph{http-agent} and requires in it's original form
(\emph{user:password}) some sort of state on the server; at least when
the system has to provide multitenancy. If instead just a general access
restriction for certain requests would suffice, no state is required.
One of the most common implementations of user-specific states is a
\emph{session} on the server, that contains one or more values
representing the state and a unique identifier, by which an entity can
be associated with. A client has to provide that session ID in order to
get provided with all the session-related data hold by the server. This
is typically done in a HTTP header, whether as \emph{Basic Auth} value,
a \emph{Cookie}, which is domain-specific, or in some other custom
header. Since the \emph{two-way authentication} (or \emph{mutual
authentication}
{[}\protect\hyperlink{ref-web_2017_wikipedia_mutual-auth}{114}{]}) is
done based on files containing keys and certificates, which are
typically not very fluctuant in it's contents or state, this procedure
is categorized as stateless. Order or origin of incoming requests have
no impact on the result of the actual authentication process. The same
applies to TLS features such as \emph{Session {[}ID, Ticket{]}
Resumption}
{[}\protect\hyperlink{ref-book_2013_networking-101_tls-session-resumption}{115}{]},
thus they are left aside, because they serve the sole purpose of
performance optimization. Similar to the \emph{Session Ticket
Resumption}
{[}\protect\hyperlink{ref-web_spec_tls-session-ticket-resumption}{116}{]}
a web token, namely the \protect\hyperlink{link_jwt}{JSON Web Token},
also moves the state towards the client, but that's about all they have
in common. A \emph{JWT} carries everything with it that's worth knowing,
including possible states, and if necessary the token is symmetrical
encrypted by the server. This is, only the server is able to obtain data
from it and reacting accordingly.

Keeping track of a state (or multiple states) on the server and keeping
data that is involved\\
synchronized between server and client is expensive and by fare trivial.
Expensive in the sense of additional resources a server would require to
remember all the data for those states, that otherwise won't be needed.
And it's not trivial, because this pattern requires the server to be
aware of all current states (sessions) and has to have them accessible
at all time. This also means, that the contents responses for certain
requests might depend on preceding requests and their incoming order.
Furthermore those session data has to be safely stored from time to
time. Otherwise if the server fails to run at some point, data only
existing in the memory would be gone without any possibility to get
recovered. To stateless authentications non of those aspects apply.
Certificates and keys as well as web tokens are both carry the
information that might be necessary with them. Thus, considering those
disadvantages, \emph{public key cryptography} and web tokens are the
preferred technologies for all authentication processes.

Except for the \emph{two-way authentication} all authentication
technologies mentioned above require an initial step to obtain some sort
of token that is used to authenticate all subsequent requests. This step
is commonly known as \emph{login} or \emph{sign in} and requires the
authorizing entity to provide some credentials consisting at least of
two parts. One part, that uniquely relates to the entity but doesn't
have to be private, and another part only the entity knows or has.
Typically that's a username or email address and a password or some
other secret bit sequence (e.g.~stored and provided by a USB stick). As
another possible secret (or unique object) an
\emph{\protect\hyperlink{link_eid-card}{eID card}} could be used.
Conceivable applications would be (1) to let the \emph{operator} login
to the \emph{PDaaS} Management Tool or

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  to approve or authorize \emph{access requests} or \emph{data access}
  attempts. How the actual login process (1) would look like partially
  depends on the \emph{eID card}'s implementation, but in general this
  use case would make sense. When considering the german implementation
  \emph{(nPA)}, accessing the management tool via desktop requires also
  a card reader, preferably with an integrated hardware keypad. Instead
  accessing the tool on a mobile device could be achieved with the
  card's RFID-capabilities, as long as the used device is able to
  communicate with the RFID-chip. Both scenarios (1+2) need the
  \emph{nPA} to have the \emph{eID} feature enabled. If a service wants
  to provide \emph{nPA}-based online authentication
  \emph{(eID-Service)}, which is defined as a non-sovereign
  \emph{(``nicht hoheitlich'')} feature, it has to comply with several
  requirements {[}\protect\hyperlink{ref-web_bsi-spec_eid}{117}{]}
  starting with applying for a permission to send a certificate signing
  request to a BerCA.\footnote{(german)
    Berechtigungszertifikate-Anbieter} This request is send from an
  \emph{eID-Server}
  {[}\protect\hyperlink{ref-web_2017_npa-eid-server}{118}{]} in order to
  get signed a public key,\\
  which previously has been generated on a dedicated and certified
  hardware. This hardware is required by the officials as part of a
  \emph{eID-Server}. The key pair - re-generated and re-signed every
  three days - is needed to establish a connection to the \emph{nPA},
  which is then used to authenticate the owner of that \emph{eID card}.
  The described procedure appears to be highly expensive (regarding
  effort, hardware costs etc.), especially because every single
  \emph{operator} would needs to go through the whole process in order
  to support this authentication method; not mentioning the uncertainty
  of the official's decision on the permission application. Another
  approach could be to integrate an external authentication provider
  supporting the \emph{nPA}, which would not only add an additional
  dependency, but would also weaken the system. All two scenarios are
  fairly similar, insofar as they would use the same mechanism to
  initially authenticate to the system, but with different intentions.
\end{enumerate}

Because of it's simplicity the concept of web tokens are fairly
straightforward to implement into the \emph{PDaaS}. But since web tokens
ensure integrity and the optional confidentiality only of their own
carriage but not the entire HTTP payload, both requirements need to be
addressed separately. Serving HTTP over \emph{TLS} solves this issue For
connections that use a web token, it should be sufficient to rely on the
public PKI that drives \emph{HTTP} over \emph{TLS}, because all
information required to authenticate is provided by the token itself.
Though, the situation is different if instead \emph{two-way
authentication} is used. For this, the system has to provide it's own
\emph{PKI} including a Certificate Authority that issues certificates
for \emph{data consumers}, because not only the \emph{endpoints} on the
\emph{PDaaS} (server) need to be certified, all \emph{data consumers}
(clients) need to present a certificate as well. Only the \emph{PDaaS}
verifies and thus determines (supervised by the \emph{operator}) who is
authorized to get access to the system. Hence the \emph{PKI} needs to be
self-contained without any external role in order to function
independently so that only invited parties can get involved.

If a public-key-based connection, performing a \emph{mutual
authentication}, establishes successfully, it implies that the identity
originating the request is valid and the integrity of the containing
data is given. Whereas on a token-based authentication every incoming
request has to carry the token so that the system can verify and
associate the request with an account. Furthermore data it not
automatically encrypted and thus integrity is not preserved.

An advantage of token-based authentication over TLS-based \emph{mutual
authentication} is that the token can be used on multiple clients at the
same time. Or an account, a token is associated with, can actually have
more than one token. Whereas during the asymmetric cryptography-based
\emph{mutual authentication} the client's private key is required. So if
it's likely that a \emph{operator} has several clients, regardless for
what purposes, then the private key has to be on those clients. Though,
a private key typically does not leave it's current system or least does
not exist in multiple systems at the same time in order to prevent
exposure, which any action of duplication implies. To reduce those
risks, it's common practice to generate a private key at that location
where it is going to be used.

The technology \emph{De-Mail} tries to ensure authenticity of an authors
identity, by embedding a legal foundation into email-based
communication. But instead of providing technically valid authenticity
by end-to-end encryption so that a recipient can truly rely on that
information, it only goes as far as legal definition and legislation
reaches. Thus it has no relevance to this work, other then the concept
of letting a server sign outgoing data, which might be the only solution
to avoid an overhead in user interaction caused by recurring events.

Computations based on asymmetric cryptography usually is slower then the
ones based on symmetric cryptography
{[}\protect\hyperlink{ref-book_2014_chapter-10-5-asym-random-number-gen}{119}{]},
but since there are no timing constrains when interacting with the
\emph{PDaaS}, regardless of whether it's external communication with
\emph{data consumers} or internal between components, parameters for
cryptographic procedures can chosen as costly as the system resources
allow them to be, thus the level of security can be increased.

\emph{Conclusions:} Based on the several requirements and distinct
advantages of the two authentication mechanisms, it is preferred to use
asymmetric cryptography in combination with \emph{HTTPS} for the
communication between the system and \emph{data consumers}, where the
system provides it's own \emph{PKI} and a token-based authentication on
top of \emph{HTTPS} and public CAs for communication between the system
and the \emph{operator}, preferable based on
\emph{\protect\hyperlink{link_jwt}{JSON Web Tokens}}, because the
session state is preserved within the token rather then having the
system itself keeping track of it. To hardening an authentication
procedure often one or more factors are added, for example an \emph{eID
card} or one-time password. This adds complexity to the procedure and
thus increases the effort that is needed to make an attack successful.
But equally it also increases the effort to support those factors in the
first place. Using multi-factor authentication is generally valued and
will be briefly noted as an optional security enhancement for the
\emph{operator role}. However detailed discussions regarding this topic
are left to follow-up work on the specification.

\hypertarget{data-reliability}{\section{Data
Reliability}\label{data-reliability}}

Within the section \protect\hyperlink{authentication}{about
authentication} it was discussed how to preserve data integrity -
referring to possible man-in-the-middle attacks and alike. Furthermore
it was described how to authenticate the different user roles so that
their identities are ensured, though, authenticity of the actual data a
\emph{PDaaS} provides has yet to be ensured. In this case, authenticity
refers to authentic and reliable (\protect\hyperlink{sa04}{S.A.04})
data, which means a) the data really represent the entity that is
associated to the originating \emph{PDaaS} and is thus owned by that
entity, and b) the data is true at that moment when the related
responses leaves the system. Since the \emph{operator} can change the
data at any point in time, this property requires a process where a
trustworthy third party has to somehow verify the reliability of the
data in question. That process on the other hand, is in direct contrast
to the discussion about the
\protect\hyperlink{authentication}{authentication system} and why it
should be designed so that it is self-contained. If instead it's not
required to provide information on the data being reliable or not, it
won't be an issue anymore. The information can be defined in a response
as an optional property. Within the request the \emph{data consumer} has
to indicate whether the response should contain information about it's
reliability or not. Depending on what data is requested, the
\emph{PDaaS} decides whether it's necessary to test for reliability or
not. Based on the procedures that are available, the data reliability
gets verified somehow.

But how does this reliability check exactly should look like? It comes
down to two general steps. The first one is matching the actual data
involved in that request against a reference data set. The second step
is optional, although important for the \emph{data consumer} in order to
evaluate the sufficiency of the provided level of reliability. It
involves the party, that also runs the first step, to confirm the result
of that audit. The result of that evaluation then gets included in the
response.

The following proposed methods are distinguish in the provided level of
reliability as well as in the amount of effort to support them and in
the possible impact to its surrounding system. Not all data points are
necessary to test for reliability. Profile data for example are more
likely to be tested, whereas consumption lists or location histories are
more or less hard to verify, because currently there is no reliable way
to verify the origin of those data sets.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  \textbf{Local Verification by matching}\\
  The probably simplest and at the same time least reliable method is to
  just look at the existing data stored in the database and matches them
  against those data that is used to create a response.
\item
  \textbf{Local Verification and signing}\\
  An electronic ID card can serve as an authentication token for the
  \emph{operator}, but it can also be utilized to verify the reliability
  of certain data. Using the german implementation \emph{(nPA)} as an
  example, the \emph{eID} feature would provide access to the owner's
  basic profile data, which thus can be used to match against those data
  points that are both, hold by that \emph{nPA} and going to be used to
  create the response. If the result of that matching procedure is
  positive, the related data then gets signed with a \emph{QES} courtesy
  of the \emph{data subject's} \emph{nPA}. That signature also gets
  included in the response, so that the recipient can verify the
  reliability of the data.
\item
  \textbf{Remote Verification and signing}\\
  Another method involves a third party who also has the same data that
  needs to be tested. The idea is to hand the data in question over to
  that party, who then tries to match against all those data points
  available in that context. The party also has the ability to sign
  data. Which is what she does, if the matching procedure has a positive
  outcome. It is required to sign the whole response or at least a
  replicable data set that contains the data that were initially
  required. The party then hands everything back to the \emph{PDaaS} for
  further processing.
\item
  \textbf{Recurring Certification}\\
  The following method describes a modification of (3). In this method
  involved no matching procedure. The external third party, verifies if
  the data in question are correct and if they relate to the \emph{data
  subject} by either literally looking at the data or by automatically
  processing a matching against their databases. If that party is
  satisfied, a certificate will be issued. This certificate contains an
  expiration date, which implies the consequence of going through this
  process again in the future, much like an issuing process of a common
  \emph{Certificate Authority}. This certificate is then served as part
  of a response, which enabled the \emph{data consumer} to verify the
  data reliability on its own. This is done by hashing the data in
  question, decrypting the hash embedded in the certificate and matching
  one against the other. If they are equal, the data has not changed
  since the party's review and is therefore reliable. If data has
  changed in the \emph{PDaaS} and data points are affected, that are
  also included in this verification process, then a new certificate
  created, because the containing hash is now invalid.
\end{enumerate}

Only one method per request should be used to verify data reliability,
because every method can imply a different level of confidence. As
described above the response send back to the \emph{data consumer} has
to indicate the method that has been used. Based on that the \emph{data
consumer} is then able evaluate that level und can act accordingly
(e.g.~verify a signature).

Expanding those verification procedures is reasonable, but to keep it
simple for now this aspect won't receive further attention, since the
current requirements are sufficiently met. It will left to future work,
though.

\emph{Conclusions:}

The signing procedure as part of local verification method involve
private key and certificate stored on the operator's \emph{eID card}.
Every time when the \emph{PDaaS} verifies data reliability that method
has to runs. Thus the \emph{operator} is forced to interact wit the
\emph{PDaaS}. Otherwise the operators private key need to be stored
somewhere within the \emph{PDaaS}. No matter where or when, that would
potentially expose a highly confidential part of a cryptographic
procedure. Not only would this reduces the overall security level of the
system, it also makes every task this method is involved vulnerable to
certain attacks. Aside from that, it's highly unlikely that an \emph{eID
card} would allow to extract it's containing private keys. This is,
increasing inconvenience is inevitable for this proposed method. The
\emph{Local Verification and signing} method also has the same
dependencies mentioned in the discussion about the requirements for
using the (german) \emph{eID card} as an authentication token. And since
it was rejected because of those dependencies and because of the
inconvenience mentioned before, this verification method eventually will
not being supported in the specification.

The \emph{Remote Verification and signing} method would require the
external party to be an official authority, because no other entity has
a) the data in question (primarily profile data), which makes them b)
legally binding. They are commonly trusted. The same goes for The
\emph{Recurring Certification}, but while the \emph{Remote Verification
and signing} method introduces a very strong dependency to that external
party, the \emph{Recurring Certification}\\
offers a simple loosely linking dependency. Whose design would make it
even possible to obtain such a certificate manually but automate it on
the other side. Nevertheless, both provide a trustworthy certification.

Finally the first method, which does just a matching of two data sets
against each other. Those data sets are obtained from the same
\emph{PDaaS} storage, but at a different time; right before the request
finally gets proceeded, though. The method is not very useful - in
general and specifically to this issue, because not much happens within
the system during that time (case were data in the storage changes
during request processing are discussed in the section on
\emph{\protect\hyperlink{access-management}{Access Management}}). Even
if the whole system would be compromised, this method has no use in that
case, because a) if that's the situation, other issues might need more
urgent addressing then ensuring data reliability for \emph{data
consumers} and b) even then, the chances are insignificant that this
method result is negative. Hence it provides the lowest level of
reliability.

Certain fields of application of a \emph{PDaaS} as a data resource might
already impose some constraints about the level of reliability and maybe
even how that can be provide. Determined by legislation or other rules,
violation might prevent the \emph{PDaaS} from being used. Others instead
- depending on their guidelines and business model - don't rely on any
kind of confidence. In general, \emph{data consumers} are expected to
already have a basic confidence in a \emph{PDaaS} and the data coming
from there. Regardless of that, providing an indication about the data's
authenticity is valued as a first and important step towards a fully
working feature. All of the proposed verification methods have some
downsides, Though, the \emph{Recurring Certification} method would be
the least invasive and therewith an adequate choice.

For the \emph{PDaaS} a primary goal is to preserve all data owned by the
\emph{data subject} and giving her control over where the data might go;
not providing sufficient proof for the data authenticity.\\
Though, it is still important, to provide \emph{data consumers} with an
information about the level of reliability, but it is up to them how to
rate that information and how to proceed with the obtained data.

\hypertarget{access-management}{\section{Access
Management}\label{access-management}}

In the subsequent section it will be discussed, how several processes
around the topic of \emph{data consumers obtaining data from the PDaaS}
can be modeled, what consequences certain variations might have and what
issues need to be addressed.

Below it is proposed what a general design might look like for the
process of how \emph{data consumers} get authorized and thereby access
the \emph{data subject's} personal data and how
\protect\hyperlink{standards-specifications-and-related-technologies}{previous
mentioned technologies} can be assembled in order to meet the specified
\protect\hyperlink{requirements}{requirements}. OR Based on the
\protect\hyperlink{standards-specifications-and-related-technologies}{outlined
technologies} and specified
\protect\hyperlink{requirements}{requirements} the general design for a
process in which a \emph{data consumer} gets authorized and thereby
access the \emph{operator's} personal data is proposed as followed.

\textbf{Part One: consumer registration}\\
0) The \emph{operator} creates a new unique URI in the system

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Prepare registration}; the \emph{operator} has to tell third
  party were and how to register as a \emph{data consumer} by handing
  over a URI that is unique to the current registration process.
  \emph{Several things need to be noted here. First, the operator
  ``pulls'' consumers into the system. This is the only way for a
  consumer to establish a relation. If consumers were able initiate this
  process on their own without the operator's involvement, it would be
  much harder for the system to detect spam or fraudulent requests.
  Second, handing over that URI must be done over a secure channel.}
\end{enumerate}

\emph{NOTICE:} the two initial steps could also be made from the
opposite direction. Third parties put all information and data required
for a registration together and present them to the \emph{operator} in
form of a QR-Code, so that the \emph{operator} can obtain it and whereby
is able to proceed. This approach would short cut and hence simplify the
process.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\item
  \textbf{Send permission request}; The third party then makes the
  actual attempt to register as a \emph{data consumer} by providing
  required information. Those information have to be include some kind
  of feedback channel (e.g.~URI) so that the system can get back to that
  third party.
\item
  \textbf{Review permission request}; the operator gets notified about
  new registration attempts, which she then has to review and decide
  whether to grant or refuse the requested data access.
\item
  \textbf{Create permission profile}; if access has been granted a new
  \emph{permission profile} is going to be created. Optionally, a new
  \emph{permission profile} could also be created if the access has been
  refused. It's just meant for the \emph{operator} to keep track of her
  decisions.
\item
  \textbf{Respond to third party}; regardless of the decision, the third
  party get's then informed via feedback channel about that decision and
  is also provided with further details required to obtain actual data.
\end{enumerate}

\textbf{Part Two: obtain data}\\
0) A successful registration as a \emph{data consumer} is required

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Send request}; \emph{data consumer} sends \emph{access
  request} to the system, containing a all information about what data
  is needed, how to process the data and what the response should
  contain.
\item
  \textbf{Parse and check request}; after the system has received an
  \emph{access request}, first it\\
  authenticates the \emph{data consumer} and checks the related
  \emph{permission profile}. According to the defined \emph{access
  rules}, the system decides how to proceed. Either it pauses, because
  it needs further attention from the \emph{operator}, or it can start
  to process and create the response.
\item
  \textbf{Compute response}; How that would look like mainly depends on
  what the \emph{access request} contains and also what the
  \emph{permission profile} determines (see \emph{access request types}
  below).
\item
  \textbf{Respond to consumer}; handover the computed response back to
  the requester. There are two ways of responding to an \emph{access
  request}. Either the system respond with a state of the process and
  where the \emph{consumer} will/can find the demanded data, or the
  \emph{consumer} includes a callback URI, which the system has to
  invoke with data in demand.
\end{enumerate}

With respect to the requirements (\protect\hyperlink{sp01}{S.P.01}),
personal data should not leak into the outside. To tackle this issue,
the following three types of \emph{access requests} are defined,
starting with the most sufficient solution:

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  \textbf{Supervised Code Execution}; \emph{access requests}
  additionally come with an executable program - binary or source code -
  potentially including information about provisioning. After the
  required data is retrieved from the storage, the program gets invoked
  with the data locally on the system but within a completely separated
  environment (\emph{sandbox}). The result of that invocation gets
  returned to the system.
\item
  \textbf{Data DRM\footnote{\emph{Digital Rights Management} - set of
    technologies, that are used to control access to data or content
    that is restricted in certain ways (e.g.~content provided by video
    streaming)}}; after data is retrieved from the storage it gets
  encrypted. The cipher is included in the response. Upfront, \emph{data
  consumers} are equipped with a small program, that can connect to the
  \emph{PDaaS} and has to wrap the \emph{consumer's} own software that
  is planned to proceed the requested data. Now when \emph{consumers}
  receive the response, the program needs to get invoked with the
  cipher, so that, by priorly fetching the key from the \emph{PDaaS},
  the cipher gets decrypted from within the invocation. Thus the data is
  made available to the wrapped software and only during runtime. After
  the invocation has finished the program needs to propagate the results
  returned by the software back to the outer environment.
\item
  \textbf{Plain Forwarding (default)}; retrieve data from the storage,
  quick-checking the result and forwarding it directly into response.
\end{enumerate}

So the data won't leave, unless the \emph{PDaaS} doesn't support any of
the proposed request types or the \emph{data consumer} provides no
alternative, so that the fallback type has to be applied. If that's the
case, the confidentiality of all personal data is already preserved,
because all communications from and to the \emph{PDaaS} are generally
happening over HTTPS anyway, so that the data is encrypted during the
transport.

The concept of authorizing a \emph{data consumer} to get the ability of
accessing personal data is fairly simple. During the \emph{registration}
consumers have to provide detailed information about their intentions,
so that the \emph{operator} is confident about their permissions when
reviewing them. The created \emph{permission profile} reflects the
result of that review. Such a \emph{permission profile} defines what
data points are requested to access and how long those permissions last.
The later is defined as \emph{permission type} and can be one of the
following:

\begin{description}
\tightlist
\item[\emph{one-time-only}]
access permissions are hereby granted for just a single \emph{access
request} (with respect to certain errors regarding the communication
layer)
\item[\emph{expires-on-date}]
access permissions are hereby granted until the defined point in time
has arrived
\item[\emph{until-further-notice}]
access permissions are hereby granted until the \emph{permission type}
has changed or the \emph{permission profile} has been deleted
\end{description}

\emph{NOTICE:} The default \emph{permission type} should be
configurable. The \emph{operator} can change all \emph{permission
profiles} at any point in time.

Among other information, an \emph{access request} contains the
\emph{data query} that shows very precisely what data points are
affected by that request. So if an \emph{access request} arrives at the
\emph{PDaaS}, assuming the \emph{data consumer} has been authenticated
sufficiently, the systems (0) searches for a \emph{permission profile}
that correspond to the \emph{data consumer} and the requested data
points. If it fails to find one, the access request gets refused. But if
it does, then it checks (1) if the permission type suffices at that
moment and (2) if the query only contains data points that are also
enabled in the \emph{profile}. Here the order does matter, because it is
imaginable that the operation behind (1) is less complex then operation
(2). So, at the end running (1) before (2) can result in a lower
response-time, if operation (1) already results negative. If all
operations have a positive result, access is granted.

As stated in the section about \protect\hyperlink{data-reliability}{data
reliability}, the \emph{data subject} is able to add, change or remove
all her data or even the \emph{permission profiles} at any point in
time. This raises the question of how to solve the situation were
\emph{data requests} are being processed, while those changes are
happening and might affect the result of those requests. The first and
simplest approach would be to not address this issue at all, but that
would be unreasonable, because providing data to the \emph{consumer}
normally means for the \emph{data subject} get something in return or to
somehow benefit from that. So that approach is no option. Using a
failure of reliability verifications as a mechanism to re-request data
won't work either in that case, because it would be based on a wrong
assumption, since that failure can have multiple causes, not only the
issue here in question. A stateless solution seems to be not fitting due
to the time-related dependency. So the only currently perceivable way is
to keep track of all momentarily processing or pending \emph{access
requests}, to detect those who are affected by that changes so that each
of them can be aborted and processed again. Here it is important to
determine the right moment, when all changes are done, otherwise the
system might end up restarting those computations repeatedly within a
short amount of time. The described issue relates to both,
\emph{personal data} and \emph{permission profiles}, because either can
impact the response send to t \emph{data consumer}. Furthermore, it
needs to be ensured that only after the \emph{permission requests} are
being reviewed and the \emph{permission profiles} are being created, the
\emph{data consumers} receive their credentials or a notification to get
started.

It is up to the \emph{data consumers} to decide which data they are
requesting to access, but how do they know what data can be requested?
The only option is to expose information about data availability, which
can be done in a variety of ways. First, those information can be made
publicly available via URI, providing a Machine-readable format, so that
it can be processed automatically by \emph{data consumers}. It is also
feasible to restrict that access to only registered \emph{consumers}, in
order to prevent those information from being crawled. They might be
valued as meta data and therefore used in unwanted computations that
could raise privacy concerns. It is imaginable to let the
\emph{operator} restrict the access to these availability information on
the level of individual \emph{data consumers} or system-wide, and to set
a default configuration for that behaviour. Depending on that
configurations request might fail, thus requester need to be provided
with meaningful errors. Http error codes
{[}\protect\hyperlink{ref-web_spec_http-error-codes}{120}{]} might be a
a sufficient fit for that purpose.

An already standardized way to implement authorization would be
\protect\hyperlink{link_oauth}{OAuth} Specification, and since the TLS
layer is already in place to handle authentication, the choice would be
to use version 2 of the standard, because it relies on HTTPS. Only two
of the four \emph{grant types} provided by OAuth would match with
process design introduced above. The types are \texttt{password} and
\texttt{client\_credentials}, which basically require identifier(s) and
secret or credentials to directly request the \texttt{token}. The other
two types define additional steps and interactions involving client
\emph{(consumer)} and user \emph{(operator)} before getting
the\texttt{token}. This would make the proposed process undesirably more
complicated. Although the proposal includes user interactions like
selecting and confirming requested permissions as well. According to the
documentations
{[}\protect\hyperlink{ref-web_spec_oauth-1a_client-reg}{121}{]}
{[}\protect\hyperlink{ref-web_spec_oauth-2_client-reg}{122}{]}, both
OAuth versions (1.0a and 2) require the client \emph{(data consumer)} to
register to the authorization server upfront (to obtain a
\texttt{client\_id}), before initializing the authorization process.
However, as stated before, the concept of the \emph{data subject}
``pulling'' a \emph{data consumer} towards the \emph{PDaaS} is preferred
over letting \emph{data consumers} try to ``push'' themselves towards
the system. The reason is to prevent unwanted applications for data
access, because they all have to get reviewed by the \emph{data
subject}. Furthermore, it is not within the scope of the OAuth
Specification to define how this should be accomplished. Thus, such step
needs to be added in addition to an entire OAuth-Flow, which might cause
otherwise avoidable overhead in user interactions. Moreover, the
proposed design does not include that step either. Instead, it is not
needed process at all, because according to the former proposed process,
identifying the client happens implicitly as a result of how the
resource owner \emph{(operator)} obtains the registration request from
the client (Part One: consumer registration, step 0 and 1). Further
investigations show that the \texttt{access\_token} semantic as from the
perspective of a resource server, which are a) authentication (does this
token exist?) and b) authorization (is this token valid and what does it
permit?), have in part already been provided by the proposed way of
using the TLS layer. Because every \emph{data consumer} has it's own
endpoint to connect with the \emph{PDaaS} and the certificate used by
the \emph{consumer} is singed by a signature that is only used for that
endpoint. This means, the \emph{consumer} is already authenticated, when
the TLS connection has successfully established. And since the endpoints
relates to the \emph{permission profiles} it would make providing an
\texttt{access\_token} to become obsolete. To summarize, implementing
OAuth would introduce several mechanisms that otherwise can be provided
by the combination of \emph{mutual authentication} in TLS, dedicated
endpoints and certification.

\emph{Conclusions:} In the preceding text, various solutions were
developed, based on which the following three solutions are at disposal:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  OAuth 1.0a and HTTP
\item
  OAuth 2 and HTTPS (public Certification and PKI)
\item
  HTTP over TLS with \emph{mutual Authentication}, private PKI,
  sub-domains as dedicated endpoints
\end{enumerate}

The solutions a) and b) require an extra step were \emph{data consumers}
would register themselves at the \emph{PDaaS}. This already needs a
secure channel to prevent man-in-the-middle attacks. Furthermore does
option a) obtain a symmetric key for creating signatures which have to
ensure confidentiality and integrity in the subsequent steps. Thereby
HTTPS is mandatory, which makes b) more suitable over a), because it's
also more flexible and easier to implement. Whereas solution c) moves
the complete authentication procedure to a different layer. It hence
results in separating authentication and authorization from each other,
leaving no remains of relation. This opens the authorization design up
to for example other implementations that might be more suitable for
certain \emph{data types}. In addition, it would only require little
effort to support the case where multiple \emph{data consumers} share
the same \emph{endpoint} and thereby the same \emph{permission
profiles}. And combining b) and c) would result in significant
redundancy, since both solutions have much overlap in the features they
are providing. Even though b) aims to be a framework for authorization.
The process description at the beginning of this section will be used as
the foundation of \emph{access management} in the \emph{PDaaS}.
Implementing OAuth based on this design would leave nothing from the
framework, but a simple request returning an identifier for it's
permissions. So there is not much benefit in using OAuth, other then
developers might be somewhat familiar with the API. This can be
addressed by a detailed specification for this project, hence c) is
preferred over b). At the end, the only suitable use case from the
specification would consists of just a request that obtains a token
after authenticating with the provided credentials. In the context of
this project OAuth doesn't really match with the rest of the design
aspects.

How the first steps of a \emph{consumer registration} are look like, is
up to the \emph{consumer}, even though the version involving a QR-Code
might result in a nicer user experience from the \emph{data subject's}
perspective. In any case, the secure channel is vital.

When obtaining personal data, at the same time preventing those data
from leakage is almost impossible, because of the nature of digital data
being able to get effortlessly copied. Nevertheless it is possible to
make it much more difficult, so that it becomes inefficient to bypass
those mechanism. At the same time it requires also some effort to
establish, run and maintain the infrastructure needed for those
mechanisms. In case of the \emph{Data DRM} proposal that effort is not
proportionate, because it requires additional infrastructure, interfaces
and cryptographic procedures, thus introduces new attack scenarios. For
now the only approach being considered, is the \emph{Supervised Code
Execution}, aside from the default forwarding. When implementing this
approach, two directions might need to be considered. Alongside the
executable program \emph{data consumers} either provide all dependencies
so that everything is bundled up, or don't provide any dependency at
all. The latter is preferred, because it reduces the amount of
potentially malicious, flawed or needless components, so that the
\emph{data subject}, supported by her \emph{PDaaS}, gains more
supervising capabilities and thus more control over her personal data.
Since the overall goal here is to prevent the \emph{data subject} from
loosing control over her data, it might also be conceivable, that
certain categories of personal data with a higher level of sensitivity
also require a least sufficient \emph{request type}. If the \emph{data
consumer} does not comply, access will be refused. Also, depending on
which category the personal data relates to, the \emph{PDaaS} might be
able to anonymize certain types of data somehow, if it's capable of
doing so all, because the \emph{consumer} at least supposedly knows what
individual is behind that \emph{PDaaS} it currently interacts with. The
field for \emph{data anonymization} is a large research area on its own,
which recently started to gains a lot of traction due to emerging
privacy concerns about \emph{big data}. Thus it will be left for future
work.

\section{Architecture}\label{architecture}

Taking all discussions in previous sections into account, the following
components might be feasible and would cover most use cases.

\begin{itemize}
\tightlist
\item
  which components can go where?
\item
  how can data be provided to a a data consumer without the data ever
  leaving the system?
\item
  where are reasonable places to locate the storage that holds the
  operators's personal data
\end{itemize}

\textbf{Webserver}

\begin{itemize}
\tightlist
\item
  to serve UI
\item
  relay to mobile device
\end{itemize}

\textbf{UI}

\begin{itemize}
\tightlist
\item
  data editor and importer

  \begin{itemize}
  \tightlist
  \item
    data type editor
  \end{itemize}
\item
  permission management

  \begin{itemize}
  \tightlist
  \item
    access history and permission profile
  \end{itemize}
\end{itemize}

\textbf{Storage/Persistence}

\begin{itemize}
\tightlist
\item
  regardless of the platform
\item
  connector
\item
  al least two storage: one for the actual data, the other for
  permission profiles, history and all the other application data
\item
  where to place the storage? local (e.g.~mobile device) or cloud
  (e.g.~hoster's infrastructure)

  \begin{itemize}
  \tightlist
  \item
    requires 24/7 uptime
  \end{itemize}
\end{itemize}

\textbf{Notification Infrastructure}

\begin{itemize}
\tightlist
\item
  websockets for web UIs
\item
  Google/Apple Notification server compatible connection for mobile apps
\end{itemize}

\textbf{Data API}

\begin{itemize}
\tightlist
\item
  essentially consists of two parts:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    checking permissions of the request
  \item
    persistence layer abstraction (graphql)
  \end{enumerate}
\item
  for external consumers

  \begin{itemize}
  \tightlist
  \item
    incoming permission requests and data access attempts
  \item
    outgoing data ()
  \end{itemize}
\item
  for internal clients (web UI, mobile device)
\end{itemize}

\textbf{OpenID} As discussed in previous sections the \emph{PDaaS} has
it's qualities to be seen and valued as a digital representation of the
individual that operates that system. Hence it could also be used to
authenticate the individual against an external platform. Although it is
priorly argued, that for authentication and security reasons the system
is designed to be self-contained. But providing those OpenID
capabilities isn't something the \emph{PDaaS} depends on, instead
external platforms would depend on the \emph{PDaaS} supporting that
feature. It is imaginable, that an OpenID instance mapping on a single
individual would simplify the authentication procedure provided by that
technology. However, it is beyond the scope of this work to either do
further investigations regarding that topic, nor elaborate on how this
might look integrate with the currently developed feature-set.

\begin{itemize}
\tightlist
\item
  regarding oAuth as authentication: Priorly users tended to reuse their
  password for different account, nowadays they but also tend to get
  tired of creating new accounts and profiles over and over again
  instead of having just one account for everything
  {[}\protect\hyperlink{ref-web_2009-success-of-facebook-connect}{123}{]}.
\end{itemize}

Thus the platform owners leave the responsibility of

Many websites and platforms understand those \emph{login-with
\$platformName} mechanisms as an outsourced service that handles all
security- and user-related tasks.

\emph{Conclusions:}

\begin{itemize}
\tightlist
\item
  distributed architecture (e.g.~notification/queue server + mobile
  device for persistence and administration)
\item
  the previously proposed authentication concept for the \emph{operator}
  role supports multiple\\
  clients and with the suggested technologies it can be implemented with
  almost no effort
\end{itemize}

\section{Data}\label{data}

\begin{itemize}
\item
  keep in mind to make it all somehow extendible, e.g.~by using and
  storing corresponding schemas
\item
  NOTE: step numbers marked with a \texttt{*} are somehow tasks which
  are happening in the background and don't require any user interaction
\end{itemize}

TODO: data types

\begin{itemize}
\item
  missing data types as candidate for a ``plugin''/``addon'' concept,
  because not every user might have the ability to model data types
  according to her needs. to have a community for this, were the best
  solutions might ``win''
\item
  Modelling
\item
  Categories (or Classes)
\item
  Types
\item
  database requirements
\item
  data Consumption (data inflow)

  \begin{itemize}
  \tightlist
  \item
    how data will get into the system
  \item
    hwo is the user able to do that, and how does it works
  \item
    manually and automated
  \end{itemize}
\item
  history

  \begin{itemize}
  \tightlist
  \item
    data versioning
  \item
    access logs
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{\emph{Conclusions}}{Conclusions}}\label{conclusions}

\section{Environment and Hosting}\label{environment-and-hosting}

\begin{itemize}
\item
  container
\item
  deployment (howto?)
\item
  ext. dependencies
\item
  container would make it easy for the \emph{remote/supervised code
  execution} approach to simply spin up a full fletched
  environment/sandbox, that can be restricted at will in any way
  (disable outgoing connections)
\end{itemize}

\emph{Conclusions:}

\section{User Interfaces}\label{user-interfaces}

\begin{itemize}
\tightlist
\item
  has to be visual?
\item
  what about an api? e.g.~for third party apps (for operator) - maybe
  self fulfilling because open source
\item
  Internal:

  \begin{itemize}
  \tightlist
  \item
    UI for Management \& Administration
  \item
    Authentication/login
  \end{itemize}
\end{itemize}

\emph{Conclusions:}

\chapter{Specification}\label{specification}

\section{Overview}\label{overview}

\begin{itemize}
\tightlist
\item
  purpose
\item
  architectural overview
\item
  short description of the whole process
\end{itemize}

\section{Components}\label{components}

\subsection{Webserver}\label{webserver}

\subsection{User Interface}\label{user-interface}

\subsection{Storage/Persistence}\label{storagepersistence}

\subsection{Notification
Infrastructure}\label{notification-infrastructure}

\subsection{Data API}\label{data-api}

\section{Data}\label{data-1}

\subsection{Structure \& Types}\label{structure-types}

\subsection{Read}\label{read}

\subsection{Write}\label{write}

\section{Protocols}\label{protocols}

\subsection{Permission Request / Consumer
Registration}\label{permission-request-consumer-registration}

\begin{itemize}
\tightlist
\item
  data to provide in a registration request:

  \begin{itemize}
  \tightlist
  \item
    CSR\footnote{Certificate signing request}
  \item
    callback URI as feedback channel
  \item
    permissions: what data for what purpose and how long
  \end{itemize}
\end{itemize}

\subsection{Data Access}\label{data-access}

TODO: detailed description of the algorithm that checks \emph{permission
profiles} according to an \emph{access request}; including all different
possible cases (multiple profiles for one consumer etc)

\subsection{Data Management}\label{data-management}

\section{APIs}\label{apis}

How do the APIs involved with the protocols look like?

\section{Security}\label{security}

\begin{itemize}
\item
  the downside of having not just parts of the personal data in
  different places (which is currently the common way to store), is in
  case of security breach, it would increase the possible damage by an
  exponential rate Thereby all data is exposed at once, instead of not
  just the parts which a single service has stored
\item
  does it matter from what origin the data request was made? how to
  check that? is the requester's server domain in the http header?
  eventually there is no way to check that, so me might need to go with
  request logging and trying to detect abnormal behaviour/occurrence
  with a learning artificial intelligence
\item
  is the consumer able to call the access request URI repeatedly and any
  time? (meaning will this be stateless or stateful?)
\item
  initial consumer registration would be done on a common and valid
  https:443 CA-certified connection. after transferring their cert to
  them as a response, all subsequent calls need to go to their own
  endpoint, defined as subdomains like
  \texttt{consumer-name.owners-notification-server.tld}
\end{itemize}

\subsection{Environment}\label{environment}

\subsection{Transport}\label{transport}

\begin{itemize}
\tightlist
\item
  communication between internal components \emph{must} be done in https
  only, but which ciphers? eventually even http/2?
\end{itemize}

\subsection{Storage}\label{storage}

\begin{itemize}
\tightlist
\item
  documents based DB instead of Relational DBS, because of
  structure/model flexibility
\item
  graphql because of it's nature to abstract a storage engine, which
  comes in handy when the actual storage gets relocated (e.g.~from a
  server to a mobile device)
\end{itemize}

\subsection{Authentication}\label{authentication-1}

\begin{itemize}
\tightlist
\item
  how should consumer authenticate?
\end{itemize}

\section{Recommendations}\label{recommendations}

\subsection{Software Dependencies}\label{software-dependencies}

\subsection{Host Environment(s)}\label{host-environments}

\chapter{Conclusion}\label{conclusion}

\section{\texorpdfstring{Ethical \& Social Impact (TODO: or
``Relevance'')}{Ethical \& Social Impact (TODO: or Relevance)}}\label{ethical-social-impact-todo-or-relevance}

\begin{itemize}
\tightlist
\item
  Regarding involving an official party to verify data reliability: The
  actual question would be, is the \emph{data subject} certain, that she
  really wants to hand over those capabilities to official authorities?
  Depending on which \emph{data consumers}, what task their are
  entrusted with and what motivation the \emph{data subject} has has in
  mind to do so, the \emph{PDaaS} might become a powerful
  \emph{``digital reflection''} and starts to get seen as a real and
  reliable representation of herself. Then the decisions made by
  \emph{data consumers} might have a big impact for the \emph{data
  subject's} life. For example a housing loan won't be granted or a
  medical treatment has been refused.
\end{itemize}

\section{Business Models \&
Monetisation}\label{business-models-monetisation}

\begin{itemize}
\tightlist
\item
  possible resulting direct or indirect business models
\item
  data subject might want to sell her data, only under her conditions.
  therefore some kind of infrastructure and process is required (such as
  payment transfer, data anonymization, market place to offer data)
\end{itemize}

\section{Challenges}\label{challenges}

\begin{itemize}
\item
  adoption rate of such technology
\item
  data reliability from the perspective of a \emph{data consumer} Since
  it is almost impossible to ensure complete reliability of all the data
  a \emph{PDaaS} has stored or might me offering, and because it is
  operated by exactly that individual, and that individual only, all
  data in question is relates to and is thereby owned by her, it, of
  cause, makes it not easy for \emph{data consumers} to trust
  \emph{PDaaS}s as resources for their business processes, but I am
  certain, that the demand for all different kinds of data exceeds the
  partial uncertainty of their reliability.
\item
  personal data leaking Preventing personal data from being leaked to
  the outside, is, especially because of the system's purpose, extremely
  hard to prevent, if not possible at all. Just by querying data from
  the storage or by physically transferring them from one location to
  another, it's already copied. It's the very nature of digital
  information technology/systems. So this cannot be defeated. It only
  can be impeded. Interestingly though, is the same approach the media
  industry for centuries is trying to make copyright infringements more
  difficult.
\end{itemize}

\section{Solutions}\label{solutions}

\begin{itemize}
\tightlist
\item
  even though \emph{OAuth} don't find it's way into this project,
  working through the standard inspired here and there a solution, for
  example using a URI as a feedback channel or TODO.
\end{itemize}

\section{Attack Scenarios}\label{attack-scenarios}

\begin{itemize}
\tightlist
\item
  single point of failure (data-wise),

  \begin{itemize}
  \tightlist
  \item
    but considering what data users already put into their social
    networks (or: thE social network: fb), they/it has already become a
    de facto data silo and is thus a single point of failure. If that
    service breaks or get down, the data from all users might be lost or
    worse (stolen). The aspect of data decentralisation achieved by
    individual data stores can be valued as positive.
  \end{itemize}
\item
  what about token stealing when using jwt?
\end{itemize}

\section{Future Work}\label{future-work}

\begin{itemize}
\item
  maybe enable the tool to play the role of an own OpenID provider?
\item
  going one step further and train machine (predictor) by our self with
  our own data
  (https://www.technologyreview.com/s/514356/stephen-wolfram-on-personal-analytics/)
\item
  finalize first draft of the spec with all core aspect included and
  outlined
\item
  developing based on that a first prototype to find flaws in the spec.
  iterate/repeat
\item
  release 1.0 (spec and example implementation)
\item
  touch on parts that were left blank
\item
  first supporting platforms
\end{itemize}

\section{Summary}\label{summary}

\begin{itemize}
\tightlist
\item
  main focus
\item
  unique features
\item
  technology stack \& standards
\item
  resources
\item
  the tool might be not a bulletproof vest, but
\end{itemize}

The work will be continued.

\hypertarget{refs}{}
\hypertarget{ref-web_2016_privacy-international-about-big-data}{}
{[}1{]} ``Big data privacy international.'' {[}Online{]}. Available:
\url{https://www.privacyinternational.org/node/8}. {[}Accessed:
15-Nov-2016{]}

\hypertarget{ref-paper_2008_discrimination-aware-data-mining}{}
{[}2{]} D. Pedreshi, S. Ruggieri, and F. Turini, ``Discrimination-aware
data mining,'' in \emph{Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining}, 2008, pp. 560--568
{[}Online{]}. Available:
\url{http://dl.acm.org/citation.cfm?id=1401959}. {[}Accessed:
03-Nov-2016{]}

\hypertarget{ref-book_2015_ethical-it-innovation_ethical-uses-of-information-and-knowledge}{}
{[}3{]} S. Spiekermann, \emph{Ethical IT Innovation: A Value-Based
System Design Approach}. CRC Press; Taylor \& Francis Group, LLC, 2015,
pp. 66--72 {[}Online{]}. Available:
\url{https://www.crcpress.com/Ethical-IT-Innovation-A-Value-Based-System-Design-Approach/Spiekermann/p/book/9781482226355}

\hypertarget{ref-paper_1996_bias-in-computer-systems}{}
{[}4{]} B. Friedman and H. Nissenbaum, ``Bias in computer systems,''
\emph{ACM Transactions on Information Systems (TOIS)}, vol. 14, no. 3,
pp. 330--347, 1996 {[}Online{]}. Available:
\url{http://dl.acm.org/citation.cfm?id=230561}. {[}Accessed:
07-Nov-2016{]}

\hypertarget{ref-wikipedia_2016_cognitive-bias}{}
{[}5{]} ``Cognitive bias,'' \emph{Wikipedia}, Oct-2016. {[}Online{]}.
Available:
\url{https://en.wikipedia.org/w/index.php?title=Cognitive_bias\&oldid=742803386}.
{[}Accessed: 08-Nov-2016{]}

\hypertarget{ref-web_2016_big-data-is-people}{}
{[}6{]} R. Lemov, ``Why big data is actually small, personal and very
human. Aeon essays,'' 16-Jun-2016. {[}Online{]}. Available:
\url{https://aeon.co/essays/why-big-data-is-actually-small-personal-and-very-human}.
{[}Accessed: 17-Nov-2016{]}

\hypertarget{ref-video_2015_big-data-and-deep-learning_discrimination}{}
{[}7{]} A. Dewes, ``C3TV - Say hi to your new boss: How algorithms might
soon control our lives.'' 29-Dec-2015. {[}Online{]}. Available:
\url{https://media.ccc.de/v/32c3-7482-say_hi_to_your_new_boss_how_algorithms_might_soon_control_our_lives\#video\&t=1538}.
{[}Accessed: 03-Nov-2016{]}

\hypertarget{ref-web_2010_projectvrm_about}{}
{[}8{]} ``ProjectVRM - about. ProjectVRM,'' 25-Feb-2010. {[}Online{]}.
Available: \url{https://blogs.harvard.edu/vrm/about/}. {[}Accessed:
09-Nov-2016{]}

\hypertarget{ref-paper_2013_the-personal-data-store-approach-to-personal-data-security_2013}{}
{[}9{]} Tom Kirkham, Sandra Winfield, Serge Ravet, and S. Kellomaki,
``The personal data store approach to personal data security,''
\emph{IEEE Security \& Privacy}, vol. 11, no. 5, pp. 12--19, 2013.

\hypertarget{ref-whitepaper_2014_mydata-a-nordic-model-for-human-centered-personal-data-management-and-processing}{}
{[}10{]} A. Poikola, K. Kuikkaniemi, and H. Honko, ``MyData -- a nordic
model for human-centered personal data management and processing,'' pp.
1--12, Jun. 2015 {[}Online{]}. Available:
\url{https://www.lvm.fi/documents/20181/859937/MyData-nordic-model/2e9b4eb0-68d7-463b-9460-821493449a63}.
{[}Accessed: 10-Nov-2016{]}

\hypertarget{ref-web_2016_meeco-how-it-works}{}
{[}11{]} ``Meeco how it works.'' {[}Online{]}. Available:
\url{https://meeco.me/how-it-works.html}. {[}Accessed: 09-Nov-2016{]}

\hypertarget{ref-repo_2016_pdaas-spec}{}
{[}12{]} ``Open specification of the concept called personal data as a
service (pdaas). GitHub.'' {[}Online{]}. Available:
\url{https://github.com/lucendio/pdaas_spec}. {[}Accessed:
11-Nov-2016{]}

\hypertarget{ref-web_2010_projectvrm-wiki_about-vrm}{}
{[}13{]} ``ProjectVRM wiki - about VRM.'' {[}Online{]}. Available:
\url{https://cyber.harvard.edu/projectvrm/Main_Page\#About_VRM}.
{[}Accessed: 11-Nov-2016{]}

\hypertarget{ref-web_2010_projectvrm-wiki_pims-example-list}{}
{[}14{]} ``ProjectVRM wiki - list of personal information management
systems.'' {[}Online{]}. Available:
\url{https://cyber.harvard.edu/projectvrm/VRM_Development_Work\#Personal_Information_Management_Systems_.28PIMS.29}.
{[}Accessed: 11-Nov-2016{]}

\hypertarget{ref-report_2014_data-brokers}{}
{[}15{]} F. T. C. USA, ``Data brokers,'' May 2014 {[}Online{]}.
Available:
\url{https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf}.
{[}Accessed: 17-Nov-2016{]}

\hypertarget{ref-whitepaper_2012_the-value-of-our-digital-identity_definition}{}
{[}16{]} J. Rose, O. Rehse, and B. Röber, ``The value of our digital
identity,'' \emph{Boston Cons. Gr}, 2012 {[}Online{]}. Available:
\url{https://www.libertyglobal.com/PDF/public-policy/The-Value-of-Our-Digital-Identity.pdf}

\hypertarget{ref-web_2016_wikipedia_intellectual-property}{}
{[}17{]} ``Outline of intellectual property,'' 11-Oct-2016.
{[}Online{]}. Available:
\url{https://en.wikipedia.org/w/index.php?title=Outline_of_intellectual_property\&oldid=743830160}.
{[}Accessed: 25-Dec-2016{]}

\hypertarget{ref-regulation_2016_eu_general-data-protection-regulation_definition}{}
{[}18{]} \emph{General data protection regulation}. 2016, p. L 119/33
{[}Online{]}. Available:
\url{http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679}

\hypertarget{ref-web_2016_wikipedia_information-privacy-law_us}{}
{[}19{]} Wikipedia, ``Information privacy law,'' 13-Nov-2016.
{[}Online{]}. Available:
\url{https://en.wikipedia.org/wiki/Information_privacy_law\#United_States}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-web_2016_data-protection-laws-in-the-us}{}
{[}20{]} I. J. (Loeb \& Loeb), ``PLC - data protection in the united
states: Overview,'' 01-Jul-2013. {[}Online{]}. Available:
\url{http://us.practicallaw.com/6-502-0467}. {[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-web_2015_white-house-releases-consumer-privacy-bill-draft}{}
{[}21{]} A. Wilhelm, ``White house drops `consumer privacy bill of
rights act' draft. TechCrunch,'' 27-Feb-2015. {[}Online{]}. Available:
\url{http://social.techcrunch.com/2015/02/27/white-house-drops-consumer-privacy-bill-of-rights-act-draft/}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-bill-draft_2015_us_consumer-privacy-bill-of-rights-act_definition}{}
{[}22{]} \emph{Administration discussion draft: Consumer privacy bill of
rights act of 2015}. 2015 {[}Online{]}. Available:
\url{https://www.whitehouse.gov/sites/default/files/omb/legislative/letters/cpbr-act-of-2015-discussion-draft.pdf}

\hypertarget{ref-rules_2016_fcc_to-protect-broadband-consumer-privacy_sensitive-types-of-data}{}
{[}23{]} \emph{Report and order}. 2016 {[}Online{]}. Available:
\url{https://transition.fcc.gov/Daily_Releases/Daily_Business/2016/db1103/FCC-16-148A1.pdf}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-rules_2016_fcc_to-protect-broadband-consumer-privacy_personally-identifiable-information}{}
{[}24{]} \emph{Notice of proposed rulemaking}. 2016 {[}Online{]}.
Available:
\url{https://apps.fcc.gov/edocs_public/attachmatch/FCC-16-39A1.pdf}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-web_2016_privacy-policies-are-mandatory-by-law}{}
{[}25{]} ``Privacy policies are mandatory by law,'' 23-Oct-2016.
{[}Online{]}. Available:
\url{https://termsfeed.com/blog/privacy-policy-mandatory-law/}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-web_2016_international-privacy-standards}{}
{[}26{]} ``International privacy standards,'' 29-Sep-2016. {[}Online{]}.
Available:
\url{https://www.eff.org/issues/international-privacy-standards}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-paper_2014_who-owns-yours-data}{}
{[}27{]} G. Rosner, ``Who owns your data?'' presented at the UbiComp
'14, september 13 - 17 2014, seattle, wa, usa, 2014, pp. 623--628
{[}Online{]}. Available:
\url{http://dl.acm.org/citation.cfm?doid=2638728.2641679}. {[}Accessed:
01-Dec-2016{]}

\hypertarget{ref-book_1987_private-ownership_definition}{}
{[}28{]} J. Grunebaum, \emph{Private ownership}. Routledge \& Kegan
Paul, 1987, p. 213.

\hypertarget{ref-regulation_2016_eu_general-data-protection-regulation_ownership}{}
{[}29{]} \emph{General data protection regulation}. 2016, p. L 119/12
{[}Online{]}. Available:
\url{http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679}

\hypertarget{ref-rules_2016_fcc_to-protect-broadband-consumer-privacy_ownership}{}
{[}30{]} \emph{Report and order}. 2016 {[}Online{]}. Available:
\url{https://transition.fcc.gov/Daily_Releases/Daily_Business/2016/db1103/FCC-16-148A1.pdf}.
{[}Accessed: 20-Nov-2016{]}

\hypertarget{ref-web_2016_facebook_terms-of-service}{}
{[}31{]} Facebook, ``Facebooks's terms of service. Statement of rights
and responsibilities,'' 30-Jan-2015. {[}Online{]}. Available:
\url{https://www.facebook.com/legal/terms}. {[}Accessed: 01-Dec-2016{]}

\hypertarget{ref-web_2016_twitter_terms-of-service}{}
{[}32{]} Twitter, ``Twitters's terms of service. Twitter terms of
service,'' 30-Sep-2016. {[}Online{]}. Available:
\url{https://twitter.com/tos\#intlTerms}. {[}Accessed: 01-Dec-2016{]}

\hypertarget{ref-web_2016_google_terms-of-service}{}
{[}33{]} Google, ``Google's terms of service. Google terms of service,''
30-Apr-2014. {[}Online{]}. Available:
\url{https://www.google.com/intl/en/policies/terms/regional.html}.
{[}Accessed: 01-Dec-2016{]}

\hypertarget{ref-web_2016_apple-icloud_terms-of-service}{}
{[}34{]} Apple, ``Apple's iClound terms and conditions. V. content and
your conduct,'' 25-Sep-2016. {[}Online{]}. Available:
\url{https://www.apple.com/legal/internet-services/icloud/en/terms.html}.
{[}Accessed: 01-Dec-2016{]}

\hypertarget{ref-web_2013_why-metadata-matters}{}
{[}35{]} ``Why metadata matters,'' 07-Jun-2013. {[}Online{]}. Available:
\url{https://www.eff.org/deeplinks/2013/06/why-metadata-matters}.
{[}Accessed: 24-Nov-2016{]}

\hypertarget{ref-web_2016_why-you-need-metadata-for-big-data-to-success}{}
{[}36{]} J. P. Stevens, ``Why you need metadata for big data success,''
06-Apr-2016. {[}Online{]}. Available:
\url{http://www.datasciencecentral.com/profiles/blogs/why-you-need-metadata-for-big-data-success}.
{[}Accessed: 24-Nov-2016{]}

\hypertarget{ref-web_2016_oxford_definition_big-data}{}
{[}37{]} ``Big data n.'' {[}Online{]}. Available:
\url{http://www.oed.com/view/Entry/18833\#eid301162177}. {[}Accessed:
11-Nov-2016{]}

\hypertarget{ref-web_2016_wikipedia_definition_big-data}{}
{[}38{]} Wikipedia, ``Big data,'' 11-Nov-2016. {[}Online{]}. Available:
\url{https://en.wikipedia.org/w/index.php?title=Big_data\&oldid=748964100}.
{[}Accessed: 11-Nov-2016{]}

\hypertarget{ref-paper_2015_big-data-analytics_a-survey}{}
{[}39{]} C.-W. Tsai, C.-F. Lai, H.-C. Chao, and A. V. Vasilakos, ``Big
data analytics: A survey,'' \emph{Journal of Big Data}, vol. 2, no. 1,
p. 21, Oct. 2015 {[}Online{]}. Available:
\url{http://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0030-3}.
{[}Accessed: 13-Nov-2016{]}

\hypertarget{ref-book-chapter_1999_Principles-of-knowledge-discovery-in-databases_introduction-to-data-mining}{}
{[}40{]} O. R. Zaïane, \emph{Principles of knowledge discovery in
databases}. 1999, pp. 1--2 {[}Online{]}. Available:
\url{https://webdocs.cs.ualberta.ca/~zaiane/courses/cmput690/notes/Chapter1/}.
{[}Accessed: 13-Nov-2016{]}

\hypertarget{ref-web_2013_big-data-collection-collides-with-privacy-concerns}{}
{[}41{]} ``Big data collection collides with privacy concerns, analysts
say. PCWorld,'' 10-Feb-2013. {[}Online{]}. Available:
\url{http://www.pcworld.com/article/2027789/big-data-collection-collides-with-privacy-concerns-analysts-say.html}.
{[}Accessed: 15-Nov-2016{]}

\hypertarget{ref-web_2016_answers-io}{}
{[}42{]} ``Answers.io. Answers.'' {[}Online{]}. Available:
\url{https://answers.io/answers}. {[}Accessed: 14-Nov-2016{]}

\hypertarget{ref-web_2016_big-data-enthusiasts-should-not-ignore}{}
{[}43{]} A. L. Burgelman, N. L. Burgelman, and NGDATA, ``Attention, big
data enthusiasts: Here's what you shouldn't ignore. WIRED.''
{[}Online{]}. Available:
\url{https://www.wired.com/insights/2013/02/attention-big-data-enthusiasts-heres-what-you-shouldnt-ignore/}.
{[}Accessed: 15-Nov-2016{]}

\hypertarget{ref-report_2001_3d-data-management-controlling-data-volume-velocity-and-variety}{}
{[}44{]} D. Laney, ``3D data management: Controlling data volume,
velocity, and variety,'' META Group, February 2001 {[}Online{]}.
Available:
\url{http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf}

\hypertarget{ref-paper_2015_big-data-for-development-a-review-of-promises-and-challenges:more-data}{}
{[}45{]} M. Hilbert, ``Big data for development: A review of promises
and challenges,'' \emph{Development Policy Review}, vol. 34, no. 1, pp.
135--174, December 2015 {[}Online{]}. Available:
\url{http://dx.doi.org/10.1111/dpr.12142}

\hypertarget{ref-web_2016_the-state-of-big-data}{}
{[}46{]} N. Davis Kho, ``The state of big data,'' 24-Feb-2016.
{[}Online{]}. Available:
\url{http://www.econtentmag.com/Articles/Editorial/Feature/The-State-of-Big-Data-108666.htm}.
{[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-web_2016_apple_customer-letter}{}
{[}47{]} T. C. (Apple's CEO), ``A message to our customers. Customer
letter,'' 16-Feb-2016. {[}Online{]}. Available:
\url{http://www.apple.com/customer-letter/}. {[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-web_2016_what-is-differential-privacy}{}
{[}48{]} M. Green, ``What is differential privacy? A few thoughts on
cryptographic engineering,'' 15-Jun-2016. {[}Online{]}. Available:
\url{https://blog.cryptographyengineering.com/2016/06/15/what-is-differential-privacy/}.
{[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-web_2016_eff_whatsapp-rolls-out-emd-to-end-encryption}{}
{[}49{]} B. Budington, ``WhatsApp rolls out end-to-end encryption to its
over one billion users,'' 07-Apr-2016. {[}Online{]}. Available:
\url{https://www.eff.org/deeplinks/2016/04/whatsapp-rolls-out-end-end-encryption-its-1bn-users}.
{[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-web_2007_introducing-google-traffic}{}
{[}50{]} ``Stuck in traffic? Insights from googlers into our products,
technology, and the google culture,'' 28-Feb-2007. {[}Online{]}.
Available:
\url{https://googleblog.blogspot.com/2007/02/stuck-in-traffic.html}.
{[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-web_2016_wikipedia_google-traffic}{}
{[}51{]} Wikipedia, ``Google traffic,'' 25-Oct-2016. {[}Online{]}.
Available:
\url{https://en.wikipedia.org/w/index.php?title=Google_Traffic\&oldid=746200591}.
{[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-graphic_2016_global-mobile-os-market-share}{}
{[}52{]} ``Global mobile OS market share.'' {[}Online{]}. Available:
\url{https://www.statista.com/statistics/266136/global-market-share-held-by-smartphone-operating-systems/}.
{[}Accessed: 18-Nov-2016{]}

\hypertarget{ref-estimating-the-locations-of-emergency-events-from-twitter-streams_2014}{}
{[}53{]} J. Ao, P. Zhang, and Y. Cao, ``Estimating the Locations of
Emergency Events from Twitter Streams,'' \emph{Procedia Computer
Science}, vol. 31, pp. 731--739, 2014 {[}Online{]}. Available:
\url{http://linkinghub.elsevier.com/retrieve/pii/S1877050914004980}.
{[}Accessed: 05-Nov-2016{]}

\hypertarget{ref-the-practice-of-predictive-analytics-in-healthcare_2013}{}
{[}54{]} G. Palem, ``The Practice of Predictive Analytics in
Healthcare,'' \emph{ResearchGate}, Apr. 2013 {[}Online{]}. Available:
\url{https://www.researchgate.net/publication/236336250_The_Practice_of_Predictive_Analytics_in_Healthcare}.
{[}Accessed: 05-Nov-2016{]}

\hypertarget{ref-data-collection-for-climate-changes_2014}{}
{[}55{]} N. Burger, B. Ghosh-Dastidar, A. Grant, G. Joseph, T. Ruder, O.
Tchakeva, and Q. Wodon, ``Data Collection for the Study on Climate
Change and Migration in the MENA Region,'' 2014 {[}Online{]}. Available:
\url{https://mpra.ub.uni-muenchen.de/56929/}. {[}Accessed:
04-Nov-2016{]}

\hypertarget{ref-graphic_2015_applications-of-big-data-in-10-industry-verticals}{}
{[}56{]} M. Gaitho, ``Applications of big data in 10 industry
verticals,'' 20-Oct-2015. {[}Online{]}. Available:
\url{https://www.simplilearn.com/big-data-applications-in-industries-article}.
{[}Accessed: 19-Nov-2016{]}

\hypertarget{ref-graphic_2012_personal-data-ecosystem}{}
{[}57{]} F. T. C. USA, ``Personal data ecosystem,'' \emph{Protecting
Consumer Privacy in an Era of Rapid Change - Recommendations for
Business and Policymakers - FTC Report}, March-2012. {[}Online{]}.
Available:
\url{https://www.ftc.gov/sites/default/files/documents/public_events/exploring-privacy-roundtable-series/personaldataecosystem.pdf}.
{[}Accessed: 17-Nov-2016{]}

\hypertarget{ref-paper_1965_moors-law}{}
{[}58{]} G. E. Moore, ``Cramming more components onto integrated
circuits,'' \emph{Electronics}, vol. 38, p. 4, Apr. 1965 {[}Online{]}.
Available:
\url{https://drive.google.com/file/d/0By83v5TWkGjvQkpBcXJKT1I1TTA/}.
{[}Accessed: 07-Dec-2016{]}

\hypertarget{ref-podcast_2015_cre-neuronale-netze}{}
{[}59{]} T. Pritlove and U. Schöneberg, \emph{Neuronale netze}. 2015
{[}Online{]}. Available: \url{https://cre.fm/cre208-neuronale-netze}.
{[}Accessed: 06-Dec-2016{]}

\hypertarget{ref-web_2016_industries-intention-to-invest-in-big-data}{}
{[}60{]} L. Columbus, ``51\% of enterprises intend to invest more in big
data,'' 22-May-2016. {[}Online{]}. Available:
\url{http://www.forbes.com/sites/louiscolumbus/2016/05/22/51-of-enterprises-intend-to-invest-more-in-big-data/}.
{[}Accessed: 07-Dec-2016{]}

\hypertarget{ref-web_2016_projectvrm_development-work}{}
{[}61{]} ``ProjectVRM - cDevelopment work. ProjectVRM,'' 28-Nov-2016.
{[}Online{]}. Available:
\url{https://cyber.harvard.edu/projectvrm/VRM_Development_Work}.
{[}Accessed: 09-Dec-2016{]}

\hypertarget{ref-web_2016_projectvrm_principles}{}
{[}62{]} ``ProjectVRM - principles. ProjectVRM,'' 28-Nov-2016.
{[}Online{]}. Available:
\url{https://cyber.harvard.edu/projectvrm/Main_Page\#VRM_Principles}.
{[}Accessed: 09-Dec-2016{]}

\hypertarget{ref-graphic_2011_architecture_components-of-organization-domain}{}
{[}63{]} The TAS3 Consortium, ``TAS3 architecture - figure 2.2: Major
components of organization domain.'' Jul. 2011 {[}Online{]}. Available:
\url{http://homes.esat.kuleuven.ac.be/~decockd/tas3/final.deliverables/pm42/TAS3_D02p1_TAS3.Architecture_final.pdf}

\hypertarget{ref-web_kantara-initiative}{}
{[}64{]} ``Kantara initiative -- join. innovate. trust.'' {[}Online{]}.
Available: \url{https://kantarainitiative.org/}. {[}Accessed:
14-Dec-2016{]}

\hypertarget{ref-paper_2014_personal-data-store-approach}{}
{[}65{]} T. Kirkham, S. Winfield, S. Ravet, and S. Kellomaki, ``The
personal data store approach to personal data security,'' \emph{IEEE
Security \& Privacy}, vol. 11, no. 5, pp. 12--19, 2013.

\hypertarget{ref-paper_2012_openpds_on-trusted-use-of-large-scale-personal-data}{}
{[}66{]} Y.-A. de Montjoye, S. S. Wang, A. Pentland, D. T. T. Anh, A.
Datta, and others, ``On the trusted use of large-scale personal data.''
\emph{IEEE Data Eng. Bull.}, vol. 35, no. 4, pp. 5--8, 2012
{[}Online{]}. Available:
\url{http://sites.computer.org/debull/a12dec/a12dec-cd.pdf\#page=7}.
{[}Accessed: 30-Oct-2016{]}

\hypertarget{ref-web_mit_openpds-safeanswers-project-page}{}
{[}67{]} ``openPDS/SafeAnswers - the privacy-preserving personal data
store.'' {[}Online{]}. Available: \url{http://openpds.media.mit.edu/}.
{[}Accessed: 14-Dec-2016{]}

\hypertarget{ref-paper_2014_openpds_protecting-privacy-of-meta-data-through-safeanswers}{}
{[}68{]} Y.-A. de Montjoye, E. Shmueli, S. S. Wang, and A. S. Pentland,
``openPDS: Protecting the privacy of metadata through SafeAnswers,''
\emph{PLoS ONE}, vol. 9, no. 7, p. e98790, Jul. 2014 {[}Online{]}.
Available: \url{http://dx.plos.org/10.1371/journal.pone.0098790}.
{[}Accessed: 30-Oct-2016{]}

\hypertarget{ref-web_microsoft_healthvault}{}
{[}69{]} ``Microsoft HealthVault. Overview.'' {[}Online{]}. Available:
\url{https://www.healthvault.com/de/en/overview}. {[}Accessed:
14-Dec-2016{]}

\hypertarget{ref-web_meeco_how-it-works}{}
{[}70{]} ``How it works meeco.'' {[}Online{]}. Available:
\url{https://meeco.me/how-it-works.html}. {[}Accessed: 14-Dec-2016{]}

\hypertarget{ref-slides_2015_meeco-case-study}{}
{[}71{]} M. Page, ``Online adver\textgreater{}sing -- booming or
broken?'' Sep-2015 {[}Online{]}. Available:
\url{https://meeco.me/assets/pdf/Meeco_Case_Study_Online_Advertising-Booming_or_Broken_Sept_2015.pdf}

\hypertarget{ref-web_industrial-data-space}{}
{[}72{]} ``The principles. Industrial data space e.V.'' {[}Online{]}.
Available: \url{http://www.industrialdataspace.org/en/the-principles/}.
{[}Accessed: 14-Dec-2016{]}

\hypertarget{ref-whitepaper_2016_industrial-data-space}{}
{[}73{]} B. Prof. Dr.-Ing. Otto, S. Prof. Dr. Auer, J. Cirullies, J.
Prof. Dr. Jürjens, N. Menz, J. Schon, and S. Dr. Wenzel, ``Industrial
data space - digital sovereignity over data.'' Fraunhofer-Gesellschaft
zur Förderung der angewandten Forschung e.V., 17-Aug-2016 {[}Online{]}.
Available:
\url{http://www.industrialdataspace.org/wp-content/uploads/2016/09/whitepaper-industrial-data-space-eng.pdf}

\hypertarget{ref-web_spec_http1}{}
{[}74{]} P. J. Leach, T. Berners-Lee, J. C. Mogul, L. Masinter, R. T.
Fielding, and J. Gettys, ``Hypertext transfer protocol -- HTTP/1.1,''
Jun-1999. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc2616}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_http2}{}
{[}75{]} M. Belshe, M. Thomson, and R. Peon, ``Hypertext transfer
protocol version 2 (HTTP/2),'' May-2015. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc7540}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_websockets}{}
{[}76{]} I. Fette and A. Melnikov, ``The WebSocket protocol,'' Dec-2011.
{[}Online{]}. Available: \url{https://tools.ietf.org/html/rfc6455}.
{[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_json}{}
{[}77{]} D. Crockford, ``The JSON data interchange format.'' ECMA
International, Oct-2013 {[}Online{]}. Available:
\url{http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf}

\hypertarget{ref-web_rfc_json}{}
{[}78{]} T. Bray, ``The JavaScript object notation (JSON) data
interchange format,'' Mar-2014. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc7159}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_2012_problem-with-oauth-for-authentication}{}
{[}79{]} J. Bradley, ``The problem with OAuth for authentication.''
28-Jan-2012. {[}Online{]}. Available:
\url{http://www.thread-safe.com/2012/01/problem-with-oauth-for-authentication.html}.
{[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_oauth-1a}{}
{[}80{]} ``OAuth core 1.0a.'' {[}Online{]}. Available:
\url{https://oauth.net/core/1.0a/}. {[}Accessed: 18-Dec-2016{]}

\hypertarget{ref-web_spec_oauth-2}{}
{[}81{]} D. Hardt, ``The OAuth 2.0 authorization framework,'' Oct-2012.
{[}Online{]}. Available: \url{https://tools.ietf.org/html/rfc6749}.
{[}Accessed: 18-Dec-2016{]}

\hypertarget{ref-web_2016_oauth-2}{}
{[}82{]} I. O. WG, ``OAuth 2.0.'' {[}Online{]}. Available:
\url{https://oauth.net/2/}. {[}Accessed: 16-Dec-2016{]}

\hypertarget{ref-web_spec_openid-connect-1}{}
{[}83{]} ``OpenID connect core 1.0 incorporating errata set 1,''
08-Nov-2014. {[}Online{]}. Available:
\url{https://openid.net/specs/openid-connect-core-1_0.html}.
{[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_json-web-token}{}
{[}84{]} J. Bradley, N. Sakimura, and M. Jones, ``JSON web token
(JWT),'' May-2015. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc7519}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_json-web-encryption}{}
{[}85{]} J. Hildebrand and M. Jones, ``JSON web encryption (JWE),''
May-2015. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc7516}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_spec_json-web-signature}{}
{[}86{]} J. Bradley, N. Sakimura, and M. Jones, ``JSON web signature
(JWS),'' May-2015. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc7515}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-paper_1976_d-h-key-exchange}{}
{[}87{]} W. Diffie and M. Hellman, ``New directions in cryptography,''
\emph{IEEE transactions on Information Theory}, vol. 22, no. 6, pp.
644--654, 1976 {[}Online{]}. Available:
\url{https://ee.stanford.edu/\%7Ehellman/publications/24.pdf}.
{[}Accessed: 11-Jan-2017{]}

\hypertarget{ref-book_2014_chapter-9-1-public-key-crypto}{}
{[}88{]} W. Stallings, ``9.1 principles of public-key cryptosystems,''
in \emph{Cryptography and network security: Principles and practice},
Seventh edition., Boston: Pearson, 2014, pp. 256--264.

\hypertarget{ref-web_spec_tls}{}
{[}89{]} T. Dierks and E. Rescorla, ``The transport layer security (TLS)
protocol version 1.2,'' Aug-2008. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc5246}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-book_2014_chapter-14-5-pki}{}
{[}90{]} W. Stallings, ``10.5 pseudorandom number generation based on an
asymmetric cipher,'' in \emph{Cryptography and network security:
Principles and practice}, Seventh edition., Boston: Pearson, 2014, pp.
443--445.

\hypertarget{ref-web_spec_x509}{}
{[}91{]} D. Cooper, S. Santesson, S. Farrell, S. Boeyen, and W. Housley
R. andPolk, ``Internet x.509 public key infrastructure certificate and
certificate revocation list (CRL) profile,'' May-2008. {[}Online{]}.
Available: \url{https://tools.ietf.org/html/rfc5280}. {[}Accessed:
11-Jan-2017{]}

\hypertarget{ref-web_spec_rest}{}
{[}92{]} T. Fielding, ``Representational state transfer (REST),'' in
\emph{Architectural styles and the design of network-based software
architectures}, University of California, Irvine, 2000, pp. 76--106
{[}Online{]}. Available:
\url{https://www.ics.uci.edu/~fielding/pubs/dissertation/fielding_dissertation.pdf}

\hypertarget{ref-web_spec_http-methods}{}
{[}93{]} P. J. Leach, T. Berners-Lee, J. C. Mogul, L. Masinter, R. T.
Fielding, and J. Gettys, ``HTTP methods,'' Jun-1999. {[}Online{]}.
Available: \url{https://tools.ietf.org/html/rfc2616\#section-9}.
{[}Accessed: 18-Dec-2016{]}

\hypertarget{ref-web_spec_graphql}{}
{[}94{]} ``GraphQL,'' Oct-2016. {[}Online{]}. Available:
\url{https://facebook.github.io/graphql/}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_w3c-tr_rdf}{}
{[}95{]} D. Beckett and B. McBride, ``RDF/XML syntax specification
(revised),'' 10-Feb-2004. {[}Online{]}. Available:
\url{https://www.w3.org/TR/REC-rdf-syntax/}. {[}Accessed: 19-Dec-2016{]}

\hypertarget{ref-web_w3c-tr_owl}{}
{[}96{]} W. O. W. Group, ``OWL 2 web ontology language document overview
(second edition),'' 11-Dec-2012. {[}Online{]}. Available:
\url{https://www.w3.org/TR/owl2-overview/}. {[}Accessed: 19-Dec-2016{]}

\hypertarget{ref-web_w3c-tr_sparql}{}
{[}97{]} S. Harris, A. Seaborne, and E. Prud'hommeaux, ``SPARQL 1.1
query language,'' 21-Mar-2013. {[}Online{]}. Available:
\url{https://www.w3.org/TR/sparql11-query/}. {[}Accessed: 19-Dec-2016{]}

\hypertarget{ref-web_w3c-draft_webid}{}
{[}98{]} ``WebID specifications.'' {[}Online{]}. Available:
\url{https://www.w3.org/2005/Incubator/webid/spec/}. {[}Accessed:
19-Dec-2016{]}

\hypertarget{ref-web_spec_solid}{}
{[}99{]} ``Solid specification,'' 03-Mar-2016. {[}Online{]}. Available:
\url{https://github.com/solid/solid-spec}. {[}Accessed: 17-Dec-2016{]}

\hypertarget{ref-web_2016_wiki_webaccesscontrol}{}
{[}100{]} ``WebAccessControl - w3c wiki.'' {[}Online{]}. Available:
\url{https://www.w3.org/wiki/WebAccessControl}. {[}Accessed:
19-Dec-2016{]}

\hypertarget{ref-web_2016_demo_databox}{}
{[}101{]} ``Databox.me.'' {[}Online{]}. Available:
\url{https://databox.me/}. {[}Accessed: 19-Dec-2016{]}

\hypertarget{ref-web_2015_cgroup-doc}{}
{[}102{]} T. Heo, ``Control group (v2) documentation,'' Oct-2015.
{[}Online{]}. Available:
\url{https://www.kernel.org/doc/Documentation/cgroup-v2.txt}.
{[}Accessed: 20-Dec-2016{]}

\hypertarget{ref-web_2016_kernel-namespace}{}
{[}103{]} ``Overview of linux namespaces,'' 12-Dec-2016. {[}Online{]}.
Available: \url{http://man7.org/linux/man-pages/man7/namespaces.7.html}.
{[}Accessed: 20-Dec-2016{]}

\hypertarget{ref-web_2016_open-container-initiative}{}
{[}104{]} ``Open container initiative.'' {[}Online{]}. Available:
\url{https://www.opencontainers.org/}. {[}Accessed: 20-Dec-2016{]}

\hypertarget{ref-web_oci-spec_runtime}{}
{[}105{]} ``Container runtime specification (v1.0.0-rc3),'' 12-Dec-2016.
{[}Online{]}. Available:
\url{https://github.com/opencontainers/runtime-spec/tree/v1.0.0-rc3}.
{[}Accessed: 20-Dec-2016{]}

\hypertarget{ref-web_oci-spec_image}{}
{[}106{]} ``Container image specification (v1.0.0-rc3),'' 30-Nov-2016.
{[}Online{]}. Available:
\url{https://github.com/opencontainers/image-spec/tree/v1.0.0-rc3}.
{[}Accessed: 20-Dec-2016{]}

\hypertarget{ref-web_2013_npa-sicherheitsdefizit}{}
{[}107{]} ``Basisleser weiterhin kritische schwachstelle des
elektronischen / neuen personalausweises. Netzpolitik.org,''
27-Aug-2013. {[}Online{]}. Available:
\url{https://netzpolitik.org/2013/basisleser-weiterhin-kritische-schwachstelle-des-elektronischen-neuen-personalausweises/}.
{[}Accessed: 05-Jan-2017{]}

\hypertarget{ref-web_2014_test-qes-support-in-npa}{}
{[}108{]} O. Stiemerling, ``Qualifizierte elektronische signatur mit dem
neuen personalausweis -- oder: QES mit nPA, ein selbstversuch.
CR-online.de blog,'' 26-Aug-2014. {[}Online{]}. Available:
\url{http://www.cr-online.de/blog/2014/08/26/qualifizierte-elektronische-signatur-mit-dem-neuen-personalausweis-oder-qes-mit-npa-ein-selbstversuch/}.
{[}Accessed: 05-Jan-2017{]}

\hypertarget{ref-web_2017_about-de-mail}{}
{[}109{]} D. B. der Bundesregierung für Informationstechnik,
``IT-beauftragter der bundesregierung de-mail.'' {[}Online{]}.
Available:
\url{http://www.cio.bund.de/Web/DE/Innovative-Vorhaben/De-Mail/de_mail_node.html}.
{[}Accessed: 06-Jan-2017{]}

\hypertarget{ref-statement_2013_de-mail}{}
{[}110{]} L. Neumann, ``Stellungnahme zum elektronischen
rechtsverkehr.'' 14-Apr-2013 {[}Online{]}. Available:
\url{https://ccc.de/system/uploads/128/original/demail_april2013.pdf}

\hypertarget{ref-book_2015_ethical-it-innovation}{}
{[}111{]} S. Spiekermann, \emph{Ethical IT Innovation: A Value-Based
System Design Approach}. CRC Press; Taylor \& Francis Group, LLC, 2015
{[}Online{]}. Available:
\url{https://www.crcpress.com/Ethical-IT-Innovation-A-Value-Based-System-Design-Approach/Spiekermann/p/book/9781482226355}

\hypertarget{ref-paper_2004_distributed-mapreduce}{}
{[}112{]} E. Dean and S. Ghemawat, ``MapRednce: Simplified data
processing on large clusters,'' 2004 {[}Online{]}. Available:
\url{https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf}.
{[}Accessed: 27-Dec-2016{]}

\hypertarget{ref-web_spec_tls-12_client-auth}{}
{[}113{]} T. Dierks, ``The transport layer security (TLS) protocol
version 1.2,'' Aug-2008. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc5246\#section-7.4.6}. {[}Accessed:
09-Jan-2017{]}

\hypertarget{ref-web_2017_wikipedia_mutual-auth}{}
{[}114{]} ``Mutual authentication,'' 02-Sep-2016. {[}Online{]}.
Available:
\url{https://en.wikipedia.org/w/index.php?title=Mutual_authentication\&oldid=737409981}.
{[}Accessed: 10-Jan-2017{]}

\hypertarget{ref-book_2013_networking-101_tls-session-resumption}{}
{[}115{]} ``Networking 101: Transport layer security (TLS) - high
performance browser networking (o'Reilly). High performance browser
networking,'' 2013. {[}Online{]}. Available:
\url{https://hpbn.co/transport-layer-security-tls/\#tls-session-resumption}.
{[}Accessed: 12-Jan-2017{]}

\hypertarget{ref-web_spec_tls-session-ticket-resumption}{}
{[}116{]} P. E. Joseph Salowey H. Zhou, ``Transport layer security (TLS)
session resumption without server-side state,'' Jan-2008. {[}Online{]}.
Available: \url{https://tools.ietf.org/html/rfc5077}. {[}Accessed:
12-Jan-2017{]}

\hypertarget{ref-web_bsi-spec_eid}{}
{[}117{]} ``BSI - technische richtlinien des BSI - BSI TR-03130
eID-server.'' {[}Online{]}. Available:
\url{https://www.bsi.bund.de/DE/Publikationen/TechnischeRichtlinien/tr03130/tr-03130.html}.
{[}Accessed: 06-Jan-2017{]}

\hypertarget{ref-web_2017_npa-eid-server}{}
{[}118{]} ``Personalausweisportal - eID-server.'' {[}Online{]}.
Available:
\url{https://www.personalausweisportal.de/DE/Wirtschaft/Technik/eID-Server/eID-Server_node.html;jsessionid=8C7F11821065F2505F22AFEF65F63DFB.2_cid334}.
{[}Accessed: 06-Jan-2017{]}

\hypertarget{ref-book_2014_chapter-10-5-asym-random-number-gen}{}
{[}119{]} W. Stallings, ``9.1 public-key infrastructure,'' in
\emph{Cryptography and network security: Principles and practice},
Seventh edition., Boston: Pearson, 2014, p. 307.

\hypertarget{ref-web_spec_http-error-codes}{}
{[}120{]} P. J. Leach, T. Berners-Lee, J. C. Mogul, L. Masinter, R. T.
Fielding, and J. Gettys, ``Hypertext transfer protocol -- HTTP/1.1,''
Jun-1999. {[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc2616\#section-10}. {[}Accessed:
20-Jan-2017{]}

\hypertarget{ref-web_spec_oauth-1a_client-reg}{}
{[}121{]} ``OAuth core 1.0a.'' {[}Online{]}. Available:
\url{https://oauth.net/core/1.0a/\#rfc.section.4.2}. {[}Accessed:
01-Nov-2016{]}

\hypertarget{ref-web_spec_oauth-2_client-reg}{}
{[}122{]} D. Hardt, ``The OAuth 2.0 authorization framework,'' Oct-2012.
{[}Online{]}. Available:
\url{https://tools.ietf.org/html/rfc6749\#section-2}. {[}Accessed:
01-Nov-2016{]}

\hypertarget{ref-web_2009-success-of-facebook-connect}{}
{[}123{]} N. Carlson, ``Facebook connect is a huge success -- by the
numbers,'' 01-Jul-2009. {[}Online{]}. Available:
\url{http://www.businessinsider.com/six-months-in-facebook-connect-is-a-huge-success-2009-7}.
{[}Accessed: 16-Dec-2016{]}

\end{document}
