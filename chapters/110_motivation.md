## Motivation



Nowadays it is difficult to find a business that does not collect data about something; 
particularly humans are the targets of choice for the *Big Data Movement* 
[@web_2016_privacy-international-about-big-data]. Since humans are all individuals, they are - more 
or less - distinct from each other. However, subsets of individuals might share a minor set of 
attributes, but the bulk is still very unique to an individual, given that the overall variety of 
attributes is fairly complex. That small amount of shared attributes might seem to be less 
important, due to the nature of inflationary occurrence, but the opposite turns out to be true. 
These similarities allow to determine the individuals who are part of a subset and the ones who 
arn't. Stereotypical patterns are applied to these subsets and thus to all relating individuals. 
Those enriched information are then used to help predicting outcomes of problems or questions 
related to these individuals. In other words, searching for causation where in best the case one 
might find correlations - or in other words *discrimination*, which

>   [...] refers to unfair or unequal treatment of people based on membership to a category or a
>   minority, without regard to individual merit. *[@paper_2008_discrimination-aware-data-mining]*

When interacting directly with each other, discrimination of human beings is still a serious issue 
in our society, but also when humans leverage computers and algorithms to uncover formerly 
unnoticed information in order to include them in their decision making. For example when 
qualifying for a loan, hiring employees, investigating crimes or renting flats. Approval or denial, 
the decision is based on computed data about the individuals in question
[@book_2015_ethical-it-innovation_ethical-uses-of-information-and-knowledge], which is nothing but 
discrimination only happening on a much larger scale but  with less effort - almost parenthetically. 
The described phenomenon is originally referred to as *Bias in computer systems*
[@paper_1996_bias-in-computer-systems]. What at first seems like machines going rouge on 
humans, is, in fact, the *cognitive bias* [@wikipedia_2016_cognitive-bias] of human nature, modeled 
into machine executable language and built to reveal the patterns their creators were looking for - 
the *"Inheritance of humanness"* [@web_2016_big-data-is-people] so to say.

In addition to the identity-defining data mentioned before, humans have the habit to create more and 
more data on a daily basis - pro-actively (e.g by writing a post) and passively (e.g by allowing 
the app to access their current location while submitting the post). As a result, already gigantic 
databases keep growing bigger and bigger, waiting to be harvested, collected, aggregated, analyzed 
and finally interpreted. The crux here is, the more data being made available 
[@video_2015_big-data-and-deep-learning_discrimination] to *mine* on, the higher the chances to 
isolate datasets (clusters) that differ from each other but are coherent within themselves. Thereby 
just by defining those datasets, instead of distinguishing on an individual level, humans are being 
reduced to these set-defining characteristics in order to fit in these clusters.

In order to lower potential discrimination, either the responsible parts in these machines need to
be erased while it's crucial at the same time to raise awareness and teach people about this issue 
of discrimination, or all the personal data need to be prevented from falling into these data silos 
in the fist place. Although both approaches are valid and should be pursued simultaneously, the 
latter will be addressed in this work.
