Introduction
==========================================



## Motivation

Nowadays it is rare to find someone who doesn't collects data about some kind of things, 
particularly humans are the targets of choice for the *Big Data Movement*. But since 
humans are all individuals, which means we are all distinct from each other - more or less.
Some of us are sharing a bit more similarities, but most are much less similar to each other.
Since these few similarities are widely shared, they should be less important, because it's 
the nature of inflationary occurrence. Instead the opposite happens to be the case. It allows
to determine who is part of a subset with a specific, and therefore shared, attribute and who 
isn't, in order to relate apparently common known stereo typical patters onto the individuals 
in that subset just to predict outcomes of the corresponding problem or question. In other words, 
searching for causation where in best case one might find correlations - or discrimination, which

>   ...in the context of civil rights law, [...] refers to unfair or unequal treatment
>   of people based on membership to a category or a minority, without regard to individual merit.
>   *[@paper_2008_discrimination-aware-data-mining_abstract]*

Not only when interacting directly with each other discrimination of human beings still seems 
to be a serious issue in our society, but also when humans leveraging computers and 
algorithms to uncover former unnoticed information in order to involve them in their decision 
making. For example when granting credits, hiring employees, investigating crimes or renting flats.
Approving or denying is all happening based on data about the affected individuals. This would all 
be in fact discrimination, but on a much larger scale and rate with less effort - and almost 
parenthetically. 

The described concept, originally known as *Bias in computer systems* 
[@paper_1996_bias-in-computer-systems] is the result of humans creating the underlying data models
and interpreting the results. The machines just revealing the patterns their creators were looking 
for. So it's up to us humans teach [TODO check 5.6 @book_2015_ethical-it-innovation]

discriminate humans related to certain data, by interpreting results of deep learning and
using neuronal networking and trying to extract information out of data bulks






In addition humans intend to create more and more data every da - pro-actively by writing a
tweet and passively by appending their current location to that tweet when submitting it.
So there are huge amount of data to collect, aggregate and analyze.

So it happens  







+   It is a matter of individual perception what's good or bad, what's armful or what's an
    advantage. Collecting, aggregating and analyzing data is foremostly just science and 
    technology. It is up to the humans how such tools are getting used and what purposes they 
    are serving. Hence it should be decided by the data creators, what's happening to their 
    data and what parts of them are used.

+   Additionally 



+   It is all about understanding the human being and why she behaves as she does. The 
    challenge is not only to computing certain motives but rather concluding to the right ones. 
    When analyzing computed results with the corresponding data models and trying to conclude, 
    it is important to keep in mind, that correlation is no proof of causation.



+   The idea here is (1) to make the individual capable of controlling her entire data distribution
    and (2) thus reducing the amount of *potentially discriminatory* attributes leaking into 
    arbitrary data silos. 



## Purpose & Outcome

+   Personal Data Store (aka. Service, Space, Vault, Cloud, Management/Manager), VRM (Vendor 
    Relation Management) aka "CRM upside down"
+   Open Source
