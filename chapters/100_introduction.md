Introduction
==========================================



## Motivation

Nowadays it is rare to find someone who doesn't collects data about some kind of things, 
particularly humans are the targets of choice for the *Big Data Movement*. But since 
humans are all individuals, which means we are all distinct from each other - more or less.
Some of us are sharing a bit more similarities, but most are much less similar to each other.
Since these few similarities are widely shared, they should be less important, because it's 
the nature of inflationary occurrence. Instead the opposite happens to be the case. It allows
to determine who is part of a subset with a specific, and therefore shared, attribute and who 
isn't, in order to relate apparently common known stereo typical patters onto the individuals 
in that subset just to predict outcomes of the corresponding problem or question. In other words, 
searching for causation where in best case one might find correlations - or so called 
*discrimination*, which

>   [...] refers to unfair or unequal treatment of people based on membership to a category or a
>   minority, without regard to individual merit. *[@paper_2008_discrimination-aware-data-mining_abstract]*

When interacting directly with each other, discrimination of human beings is still a serious issue 
in our society, but also when humans leveraging computers and algorithms to uncover former 
unnoticed information in order to include them in their decision making. For example when 
granting credits, hiring employees, investigating crimes or renting flats. Approving or denying 
is all happening based on data about the affected individuals
[@book_2015_ethical-it-innovation_ethical-uses-of-information-and-knowledge], which is nothing
but discrimination, but on a much larger scale and with less effort - almost parenthetically. 
The described phenomenon is originally referred as *Bias in computer systems*
[@paper_1996_bias-in-computer-systems]. What at fist seems to look like machines going rouge on 
humans, is in fact the *cognitive bias* [@wikipedia_2016_cognitive-bias] of the human nature, 
modeled in machine executable language and made to reveal the patterns their creators were looking 
for.

In addition to the identity-defining data mentioned above humans intend also to create more and 
more data on a daily basis - pro-actively e.g by writing a tweet and passively e.g by appending 
their current location to that tweet when submitting it.
As a result there are huge amount of data to collect, aggregate and analyze.

So it happens  







+   It is a matter of individual perception what's good or bad, what's armful or what's an
    advantage. Collecting, aggregating and analyzing data is foremostly just science and 
    technology. It is up to the humans how such tools are getting used and what purposes they 
    are serving. Hence it should be decided by the data creators, what's happening to their 
    data and what parts of them are used.

+   Additionally 



+   It is all about understanding the human being and why she behaves as she does. The 
    challenge is not only to computing certain motives but rather concluding to the right ones. 
    When analyzing computed results with the corresponding data models and trying to conclude, 
    it is important to keep in mind, that correlation is no proof of causation.

+   To tackle the issue of potential discrimination by human-made algorithms


+   The idea here is (1) to make the individual capable of controlling her entire data distribution
    and (2) thus reducing [@video_2015_big-data-and-deep-learning_discrimination] the amount 
    of *potentially discriminatory* [@paper_2008_discrimination-aware-data-mining] attributes 
    leaking into arbitrary calculations. 



## Purpose & Outcome

+   Personal Data Store (aka. Service, Space, Vault, Cloud, Management/Manager), VRM (Vendor 
    Relation Management) aka "CRM upside down"
+   Open Source
